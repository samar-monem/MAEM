{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"test_fold=4 #choose fold from[0,1,2,3,4] ","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:25:20.477856Z","iopub.execute_input":"2024-03-18T13:25:20.478703Z","iopub.status.idle":"2024-03-18T13:25:20.489393Z","shell.execute_reply.started":"2024-03-18T13:25:20.478661Z","shell.execute_reply":"2024-03-18T13:25:20.488301Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# !pip install openpyxl \n!pip install xlrd==2.0.1 \n!pip install Pandas==1.3.5\n# !pip install Pandas==1.1.5\n!pip install  gdown\n!pip install deepchem\n!pip install pysmiles\n\nimport tensorflow as tf\n\nimport numpy as np\nimport pandas as pd\n \n\n\n\ndef normalize1(X, means1=None, std1=None, means2=None, std2=None, feat_filt=None, norm='tanh_norm'):\n    if std1 is None:\n        std1 = np.nanstd(X, axis=0)\n    if feat_filt is None:\n        feat_filt = std1!=0\n    X = X[:,feat_filt]\n    X = np.ascontiguousarray(X)\n    if means1 is None:\n        means1 = np.mean(X, axis=0)\n    X = (X-means1)/std1[feat_filt]\n    if norm == 'norm':\n        return(X, means1, std1, feat_filt)\n    elif norm == 'tanh':\n        return(np.tanh(X), means1, std1, feat_filt)\n    elif norm == 'tanh_norm':\n        X = np.tanh(X)\n        if means2 is None:\n            means2 = np.mean(X, axis=0)\n        if std2 is None:\n            std2 = np.std(X, axis=0)\n        X = (X-means2)/std2\n        X[:,std2==0]=0\n        return(X, means1, std1, means2, std2, feat_filt) \n\n\n    \ndef get_data():\n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  smiles=pd.read_excel('pubchem.xls', header=None)\n  smiles=np.array(smiles)\n\n\n  !gdown 10ztxKtGSVU7p9yPoCsTnapVLThWpjjYT\n  data_to_repeat=pd.read_excel('oneil.xlsx', header=None)\n  data_to_repeat=np.array(data_to_repeat)\n  \n\n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  unique_drugs=pd.read_excel('pubchem.xls', header=None)\n  unique_drugs=np.array(unique_drugs)         \n    \n  !gdown 15yf5KKQqRROygqB05MZqfztmf5elTBSw\n  feature_cell=pd.read_csv('vae_expression256.csv')  \n  feature_cell=np.array(feature_cell)\n    \n  !gdown  14upx46iIPcO80y_tcvbaS3uwMkZfKLVw\n  redkit_drug=pd.read_excel('redkit_drug.xlsx',header=None)\n  unique_drugs1=np.array(redkit_drug)\n    \n  !gdown 1-qSA_3RF6eBSgSckcV5u1dORTBTia-lq\n  graph=pd.read_excel('views0b123.xlsx',header=None)\n  graph=np.array(graph)\n  \n  !gdown 1A_E0ek950T1r8OAH2EqLTdNKzIWv7K6_\n  target=pd.read_csv('drug_target.csv')\n  target=np.array(target)\n\n  !gdown 1zDRCKuJUkAxATHjzWFEUc20o_4u4nDwJ\n  copy=pd.read_csv('vae_copy256.csv')\n  copy=np.array(copy)\n\n  !gdown 1dQm1MQp-qh-jTweXq81HJ7zgjfYxqKyg\n  mutation=pd.read_csv('vae_mutation256.csv')\n  mutation=np.array(mutation)\n    \n  !gdown 1BV5XFD8KZB7sYLYx4YQ0uww_FhFstkru\n  protematics=pd.read_csv('vae_promatics256.csv')\n  protematics=np.array(protematics)\n\n  !gdown  1zPeJOMpLEcmgKhUANm8clSIfghvSCyrC\n  finger=pd.read_csv('drug_finger.csv')\n  finger=np.array(finger)\n\n\n  return smiles[1:,1],data_to_repeat[1:,:],unique_drugs[1:,0],feature_cell,redkit_drug,graph,unique_drugs1,target,copy,mutation,protematics,finger\n\n\ndef repeat_smiles1(data_to_repeat,unique_drugs,feature,unique_cell,finger_drug,graph,target,copy,mutation,protematics):\n#   unique_finger_feature=finger_drug[:,1:]\n#   unique_finger_name=finger_drug[:,0]\n  unique_feature=unique_cell[:,1:]\n  unique_name=unique_cell[:,0]\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  c=data_to_repeat[:,2]\n  unique_drugs=feature[:,0]\n  feature=feature[:,1:]  \n  f_drug1=[]\n  f_drug2=[]\n  feature_cell=[]\n  finger_d1=[]\n  finger_d2=[]\n  graph1=[]\n  graph2=[]\n  target1=[]\n  target2=[]\n  ccopy=[]\n  cmutation=[]\n  cprotematics=[]\n\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    cc=c[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    cc1=[m for m, v in enumerate(unique_name) if cc in v]\n#     r1= [m for m, v in enumerate(unique_redkit_name) if n1 in v]\n#     r2=[m for m, v in enumerate(unique_redkit_name) if n2 in v]\n    f_drug1.append(feature[k1[0]])\n    f_drug2.append(feature[k2[0]])\n    graph1.append(graph[k1[0]])\n    graph2.append(graph[k2[0]])\n    finger_d1.append(finger_drug[k1[0]])\n    finger_d2.append(finger_drug[k2[0]])\n    target1.append(target[k1[0]])\n    target2.append(target[k2[0]])\n    \n    feature_cell.append(unique_feature[cc1[0]])\n    ccopy.append(copy[cc1[0]])\n    cmutation.append(mutation[cc1[0]])\n    cprotematics.append(protematics[cc1[0]])\n\n  return f_drug1,f_drug2,feature_cell,finger_d1,finger_d2,graph1,graph2,target1,target2,ccopy,cmutation,cprotematics\n\n\n\n\ndef train_test_input(f_drug1,f_drug2,cell_line,index_train,index_test,synery,class1,finger_d1,finger_d2,graph1,graph2,target1,target2,copy,mutation,protematics):\n  train_f_drug1=[]\n  train_f_drug2=[]\n  train_cell_line=[]\n  train_synergy=[]\n  train_class=[]\n  train_finger_d1=[]\n  train_finger_d2=[]\n  test_f_drug1=[]\n  test_f_drug2=[]\n  test_cell_line=[]\n  test_synergy=[]\n  test_class=[]\n  test_finger_d1=[]\n  test_finger_d2=[]\n  train_graph1=[]\n  test_graph1=[]\n  train_graph2=[]\n  test_graph2=[]\n    \n  train_target1=[]\n  train_target2=[]\n  train_copy=[]\n  train_mutation=[]\n  train_protematics=[]\n  test_target1=[]\n  test_target2=[]\n  test_copy=[]\n  test_mutation=[]\n  test_protematics=[]\n  for i in range(len(index_train)):\n      \n      train_f_drug1.append(f_drug1[index_train[i]])\n      train_f_drug2.append(f_drug2[index_train[i]])\n      train_cell_line.append(cell_line[index_train[i]])\n      train_synergy.append(synergy[index_train[i]])\n      train_class.append(class1[index_train[i]])\n      train_finger_d1.append(finger_d1[index_train[i]])\n      train_finger_d2.append(finger_d2[index_train[i]])\n      train_graph1.append(graph1[index_train[i]])\n      train_graph2.append(graph2[index_train[i]])\n     \n      train_target1.append(target1[index_train[i]]) \n      train_target2.append(target2[index_train[i]])\n      train_copy.append(copy[index_train[i]])\n      train_mutation.append(mutation[index_train[i]])\n      train_protematics.append(protematics[index_train[i]])\n \n  for ii in range(len(index_test)):\n      \n      test_f_drug1.append(f_drug1[index_test[ii]])\n      test_f_drug2.append(f_drug2[index_test[ii]])\n      test_cell_line.append(cell_line[index_test[ii]])\n      test_synergy.append(synergy[index_test[ii]])\n      test_class.append(class1[index_test[ii]])\n      test_finger_d1.append(finger_d1[index_test[ii]])\n      test_finger_d2.append(finger_d2[index_test[ii]])\n      test_graph1.append(graph1[index_test[ii]])\n      test_graph2.append(graph2[index_test[ii]])\n        \n      test_target1.append(target1[index_test[ii]])\n      test_target2.append(target2[index_test[ii]])\n      test_copy.append(copy[index_test[ii]])\n      test_mutation.append(mutation[index_test[ii]])\n      test_protematics.append(protematics[index_test[ii]])\n\n  return train_f_drug1,train_f_drug2,train_cell_line,test_f_drug1,test_f_drug2,test_cell_line,train_synergy,train_class,test_synergy,test_class,train_finger_d1,train_finger_d2,test_finger_d1,test_finger_d2,train_graph1,test_graph1,train_graph2,test_graph2,train_target1,train_target2,train_copy,train_mutation,train_protematics,test_target1,test_target2,test_copy,test_mutation,test_protematics\n\n\n\ndef preprocess(index_train,index_test):\n    index_train1=[]\n    index_test1=[]\n    index_train2=(index_train)[0]\n    index_test2=(index_test)[0]\n    for i in range(len((index_train2))):\n        index_train1.append((index_train2[i]))\n        \n    for ii in range(len(index_test2)):\n        index_test1.append((index_test2[ii]))\n        \n    return index_train1,index_test1\n\ndef get_data_me2(s):\n    \n    \n\n    !gdown 1C7Z2ziPdQVzH3omIdIfyJa7VUmog4IIk\n    labels = pd.read_csv('oneil.csv', index_col=0) \n    \n    h=len(np.array(labels))\n    #labels are duplicated for the two different ways of ordering in the data\n    labels = pd.concat([labels, labels]) \n    \n    test_fold =s\n   \n    idx_train = np.where(labels['fold']!=test_fold)\n    \n\n    idx_test = np.where(labels['fold']==test_fold)\n#     \n#    \n    return idx_train,idx_test\n\n\n\n\n\ndef convert_tobin(cc):\n    cb=[]\n    for i in range(len(cc)):\n        if(cc[i]>=0.5):\n            cb.append(1)\n        else:\n            cb.append(0)\n    return cb\n\n\ndef norm1(train_cell_line,test_cell_line,norm=\"tanh_norm\"):\n# norm = \"norm\"\n    if norm == \"tanh_norm\":\n        train_cell_line, mean, std, mean2, std2, feat_filt = normalize1(train_cell_line, norm=norm)\n        test_cell_line, mean, std, mean2, std2, feat_filt = normalize1(test_cell_line, mean, std, mean2, std2, \n                                                              feat_filt=feat_filt, norm=norm)\n    else:\n        train_cell_line, mean, std, feat_filt = normalize1(train_cell_line, norm=norm)\n        test_cell_line, mean, std, feat_filt = normalize1(test_cell_line, mean, std, feat_filt=feat_filt, norm=norm)\n    \n    return train_cell_line,test_cell_line\n\n\n\n\n    \n     \n\nsmiles,data_to_repeat,unique_drugs,unique_cell,redkit_drug,graph,feature,target,copy,mutation,protematics,finger=get_data()\n\n\n\ndata_to_repeat=np.r_[data_to_repeat,data_to_repeat]\nl=int((data_to_repeat.shape[0])/2)\ndata_to_repeat[l:,0]=data_to_repeat[0:l,1]\ndata_to_repeat[l:,1]=data_to_repeat[0:l,0]\nsynergy=data_to_repeat[:,3]\n\nclass1=[]\nfor i in range(len(synergy)):\n if(synergy[i]>=30):\n    class1.append(1)\n elif(synergy[i]<0): \n    class1.append(0)\n else:\n    class1.append(2)    \n\n\nf_drug1,f_drug2,feature_cell,finger_d1,finger_d2,graph1,graph2,target1,target2,copy,mutation,protematics=repeat_smiles1(data_to_repeat,unique_drugs,feature,unique_cell,finger,graph,target,copy,mutation,protematics)\n\n\n\n\nindex_train,index_test=get_data_me2(test_fold)\n\nindex_train,index_test=preprocess(index_train,index_test)\ntrain_f_drug1,train_f_drug2,train_cell_line,test_f_drug1,test_f_drug2,test_cell_line,train_synergy,train_class,test_synergy,test_class,train_finger_d1,train_finger_d2,test_finger_d1,test_finger_d2,train_graph1,test_graph1,train_graph2,test_graph2,train_target1,train_target2,train_copy,train_mutation,train_protematics,test_target1,test_target2,test_copy,test_mutation,test_protematics=train_test_input(f_drug1,f_drug2,feature_cell,index_train,index_test,synergy,class1,finger_d1,finger_d2,graph1,graph2,target1,target2,copy,mutation,protematics)\ntrain_cell_line=np.array(train_cell_line).astype(float)\ntest_cell_line=np.array(test_cell_line).astype(float)  \ntrain_f_drug1=np.array(train_f_drug1).astype(float)\ntest_f_drug1=np.array(test_f_drug1).astype(float)\ntrain_f_drug2=np.array(train_f_drug2).astype(float)\ntest_f_drug2=np.array(test_f_drug2).astype(float)\n\ntrain_graph1=np.array(train_graph1).astype(float)\ntest_graph1=np.array(test_graph1).astype(float) \ntrain_graph2=np.array(train_graph2).astype(float)\ntest_graph2=np.array(test_graph2).astype(float)\n\ntrain_finger_d1=np.array(train_finger_d1).astype(float)\ntrain_finger_d2=np.array(train_finger_d2).astype(float) \ntest_finger_d1=np.array(test_finger_d1).astype(float)\ntest_finger_d2=np.array(test_finger_d2).astype(float)\n\n\n# train_target1=np.array(train_target1).astype(float)\n# train_target2=np.array(train_target2).astype(float)\ntrain_copy=np.array(train_copy).astype(float)\ntrain_mutation=np.array(train_mutation).astype(float)\ntrain_protematics=np.array(train_protematics).astype(float)\n# test_target1=np.array(test_target1).astype(float)\n# test_target2=np.array(test_target2).astype(float)\ntest_copy=np.array(test_copy).astype(float)\ntest_mutation=np.array(test_mutation).astype(float)\ntest_protematics=np.array(test_protematics).astype(float)\n\ntrain_finger_d1,test_finger_d1=norm1(train_finger_d1,test_finger_d1)\ntrain_finger_d2,test_finger_d2=norm1(train_finger_d2,test_finger_d2)\n\n\n\ntrain_cell_line,test_cell_line=norm1(train_cell_line,test_cell_line)\n\ntrain_f_drug1,test_f_drug1=norm1(train_f_drug1,test_f_drug1)\ntrain_f_drug2,test_f_drug2=norm1(train_f_drug2,test_f_drug2)\n\ntrain_graph1,test_graph1=norm1(train_graph1,test_graph1)\ntrain_graph2,test_graph2=norm1(train_graph2,test_graph2)\n\n# train_target1,test_target1=norm1(train_target1,test_target1)\n# train_target2,test_target1=norm1(train_target2,test_target2)\n\ntrain_copy,test_copy=norm1(train_copy,test_copy)\ntrain_mutation,test_mutation=norm1(train_mutation,test_mutation)\ntrain_protematics,test_protematics=norm1(train_protematics,test_protematics)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-18T13:25:20.536030Z","iopub.execute_input":"2024-03-18T13:25:20.536353Z","iopub.status.idle":"2024-03-18T13:27:48.861429Z","shell.execute_reply.started":"2024-03-18T13:25:20.536325Z","shell.execute_reply":"2024-03-18T13:27:48.860245Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting xlrd==2.0.1\n  Obtaining dependency information for xlrd==2.0.1 from https://files.pythonhosted.org/packages/a6/0c/c2a72d51fe56e08a08acc85d13013558a2d793028ae7385448a6ccdfae64/xlrd-2.0.1-py2.py3-none-any.whl.metadata\n  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\nDownloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: xlrd\nSuccessfully installed xlrd-2.0.1\nCollecting Pandas==1.3.5\n  Obtaining dependency information for Pandas==1.3.5 from https://files.pythonhosted.org/packages/ff/7a/1ce22f0f009ee31878f717bd5b3221e993a7ebc02391d7a315982c2224dc/pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from Pandas==1.3.5) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from Pandas==1.3.5) (2023.3)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from Pandas==1.3.5) (1.24.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7.3->Pandas==1.3.5) (1.16.0)\nDownloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: Pandas\n  Attempting uninstall: Pandas\n    Found existing installation: pandas 2.0.3\n    Uninstalling pandas-2.0.3:\n      Successfully uninstalled pandas-2.0.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nbeatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\nesda 2.5.1 requires pandas>1.4, but you have pandas 1.3.5 which is incompatible.\nfeaturetools 1.28.0 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\nfitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 1.3.5 which is incompatible.\ngeopandas 0.14.1 requires pandas>=1.4.0, but you have pandas 1.3.5 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\nspopt 0.6.0 requires pandas!=1.5.0,>=1.4, but you have pandas 1.3.5 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nwoodwork 0.27.0 requires pandas>=1.4.3, but you have pandas 1.3.5 which is incompatible.\nxarray 2023.12.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Pandas-1.3.5\nCollecting gdown\n  Obtaining dependency information for gdown from https://files.pythonhosted.org/packages/cb/56/f4845ed78723a4eb8eb22bcfcb46e1157a462c78c0a5ed318c68c98f9a79/gdown-5.1.0-py3-none-any.whl.metadata\n  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.11.17)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.1.0-py3-none-any.whl (17 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.1.0\nCollecting deepchem\n  Obtaining dependency information for deepchem from https://files.pythonhosted.org/packages/fe/47/5f308db61d120e7d05448a298bbf5d765ddf1c4aafc3340064b131667067/deepchem-2.7.1-py3-none-any.whl.metadata\n  Downloading deepchem-2.7.1-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.3.2)\nRequirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.24.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.3.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.2.2)\nCollecting scipy<1.9 (from deepchem)\n  Obtaining dependency information for scipy<1.9 from https://files.pythonhosted.org/packages/bc/fe/72b611ba221c3367b06163992af4807515d6e0e09b3b9beee8ec22162d6f/scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\nCollecting rdkit (from deepchem)\n  Obtaining dependency information for rdkit from https://files.pythonhosted.org/packages/b1/5d/570131a2ccf83e42efe3d601cf7054cb8b0ec0b390d775813fca76af9d7b/rdkit-2023.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading rdkit-2023.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2023.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from rdkit->deepchem) (10.1.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->deepchem) (3.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.16.0)\nDownloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading rdkit-2023.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy, rdkit, deepchem\n  Attempting uninstall: scipy\n    Found existing installation: SciPy 1.11.4\n    Uninstalling SciPy-1.11.4:\n      Successfully uninstalled SciPy-1.11.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\nesda 2.5.1 requires pandas>1.4, but you have pandas 1.3.5 which is incompatible.\nesda 2.5.1 requires scipy>=1.9, but you have scipy 1.8.1 which is incompatible.\nfeaturetools 1.28.0 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\nfeaturetools 1.28.0 requires scipy>=1.10.0, but you have scipy 1.8.1 which is incompatible.\nfitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 1.3.5 which is incompatible.\njax 0.4.21 requires scipy>=1.9, but you have scipy 1.8.1 which is incompatible.\njaxlib 0.4.21+cuda11.cudnn86 requires scipy>=1.9, but you have scipy 1.8.1 which is incompatible.\nkaggle-environments 1.14.3 requires scipy>=1.11.2, but you have scipy 1.8.1 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.8.1 which is incompatible.\nspopt 0.6.0 requires pandas!=1.5.0,>=1.4, but you have pandas 1.3.5 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nwoodwork 0.27.0 requires pandas>=1.4.3, but you have pandas 1.3.5 which is incompatible.\nwoodwork 0.27.0 requires scipy>=1.10.0, but you have scipy 1.8.1 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed deepchem-2.7.1 rdkit-2023.9.5 scipy-1.7.3\nCollecting pysmiles\n  Obtaining dependency information for pysmiles from https://files.pythonhosted.org/packages/cb/ed/08c83b4834b1a46bf7684616b692cd0a1009371136c8f84be719dc9d4b08/pysmiles-1.1.2-py2.py3-none-any.whl.metadata\n  Downloading pysmiles-1.1.2-py2.py3-none-any.whl.metadata (10 kB)\nCollecting pbr (from pysmiles)\n  Obtaining dependency information for pbr from https://files.pythonhosted.org/packages/64/dd/171c9fb653591cf265bcc89c436eec75c9bde3dec921cc236fa71e5698df/pbr-6.0.0-py2.py3-none-any.whl.metadata\n  Downloading pbr-6.0.0-py2.py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pysmiles) (3.1)\nDownloading pysmiles-1.1.2-py2.py3-none-any.whl (22 kB)\nDownloading pbr-6.0.0-py2.py3-none-any.whl (107 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.5/107.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pbr, pysmiles\nSuccessfully installed pbr-6.0.0 pysmiles-1.1.2\n","output_type":"stream"},{"name":"stderr","text":"<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","output_type":"stream"},{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 67.8MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=10ztxKtGSVU7p9yPoCsTnapVLThWpjjYT\nTo: /kaggle/working/oneil.xlsx\n100%|█████████████████████████████████████████| 850k/850k [00:00<00:00, 114MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 63.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=15yf5KKQqRROygqB05MZqfztmf5elTBSw\nTo: /kaggle/working/vae_expression256.csv\n100%|█████████████████████████████████████████| 197k/197k [00:00<00:00, 114MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=14upx46iIPcO80y_tcvbaS3uwMkZfKLVw\nTo: /kaggle/working/redkit_drug.xlsx\n100%|██████████████████████████████████████| 68.6k/68.6k [00:00<00:00, 99.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-qSA_3RF6eBSgSckcV5u1dORTBTia-lq\nTo: /kaggle/working/views0b123.xlsx\n100%|██████████████████████████████████████| 42.1k/42.1k [00:00<00:00, 72.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1A_E0ek950T1r8OAH2EqLTdNKzIWv7K6_\nTo: /kaggle/working/drug_target.csv\n100%|██████████████████████████████████████| 46.4k/46.4k [00:00<00:00, 77.9MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1zDRCKuJUkAxATHjzWFEUc20o_4u4nDwJ\nTo: /kaggle/working/vae_copy256.csv\n100%|████████████████████████████████████████| 110k/110k [00:00<00:00, 95.4MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1dQm1MQp-qh-jTweXq81HJ7zgjfYxqKyg\nTo: /kaggle/working/vae_mutation256.csv\n100%|█████████████████████████████████████████| 110k/110k [00:00<00:00, 101MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1BV5XFD8KZB7sYLYx4YQ0uww_FhFstkru\nTo: /kaggle/working/vae_promatics256.csv\n100%|█████████████████████████████████████████| 110k/110k [00:00<00:00, 100MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1zPeJOMpLEcmgKhUANm8clSIfghvSCyrC\nTo: /kaggle/working/drug_finger.csv\n100%|██████████████████████████████████████| 13.2k/13.2k [00:00<00:00, 48.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1C7Z2ziPdQVzH3omIdIfyJa7VUmog4IIk\nTo: /kaggle/working/oneil.csv\n100%|████████████████████████████████████████| 988k/988k [00:00<00:00, 84.6MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow==2.13.0\n!pip install spektral\nfrom tensorflow import keras\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Minimum,Maximum,Add,Maximum,PReLU, Flatten,Reshape,Dropout, Input,Dense,Add,concatenate,BatchNormalization, Activation,Lambda#,MultiHeadAttention,AdditiveAttention\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.metrics import confusion_matrix, mean_squared_error,mean_absolute_error,r2_score#,AUC\nfrom scipy.stats import pearsonr,spearmanr\nfrom sklearn.metrics import roc_curve,auc,accuracy_score,precision_score,cohen_kappa_score,precision_recall_curve,average_precision_score,roc_auc_score\nfrom tensorflow.keras import regularizers\nnp.random.seed(10)\n\n# train_cell_line=tf.convert_to_tensor(train_cell_line)\n# test_cell_line=tf.convert_to_tensor(test_cell_line)\ntrain_synergy=tf.convert_to_tensor(train_synergy)\ntrain_class=tf.convert_to_tensor(train_class)\ntest_synergy=tf.convert_to_tensor(test_synergy)\ntest_class=tf.convert_to_tensor(test_class)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:27:48.863945Z","iopub.execute_input":"2024-03-18T13:27:48.864271Z","iopub.status.idle":"2024-03-18T13:28:18.489023Z","shell.execute_reply.started":"2024-03-18T13:27:48.864241Z","shell.execute_reply":"2024-03-18T13:28:18.487842Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow==2.13.0 in /opt/conda/lib/python3.10/site-packages (2.13.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (3.9.0)\nRequirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (2.13.1)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (16.0.6)\nRequirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (1.24.3)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (68.1.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (1.16.0)\nRequirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (2.13.0)\nRequirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (2.13.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (2.3.0)\nRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (4.5.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (1.15.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.13.0) (0.34.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.13.0) (0.41.2)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.22.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.4.4)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.13.0) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0) (3.2.2)\nCollecting spektral\n  Obtaining dependency information for spektral from https://files.pythonhosted.org/packages/ae/4c/b1149deb49c48d58bfc2b68fe6fc502d6c896c69eaf9997de45ad24b6fca/spektral-1.3.1-py3-none-any.whl.metadata\n  Downloading spektral-1.3.1-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from spektral) (1.3.2)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from spektral) (4.9.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from spektral) (3.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from spektral) (1.24.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from spektral) (1.3.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from spektral) (2.31.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from spektral) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from spektral) (1.7.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from spektral) (4.66.1)\nRequirement already satisfied: tensorflow>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from spektral) (2.13.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (3.9.0)\nRequirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.13.1)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (16.0.6)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (68.1.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.16.0)\nRequirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.13.0)\nRequirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.13.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.3.0)\nRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (4.5.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.15.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.34.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from pandas->spektral) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from pandas->spektral) (2023.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (2023.11.17)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->spektral) (3.2.0)\nCollecting numpy (from spektral)\n  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/b0/f4/d67c8c39efe3c45dfd32bb2a3fc49cbbe5496e575cc42b8bac60fe7b6701/numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.41.2)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.2.0->spektral) (2.22.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.2.0->spektral) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.2.0->spektral) (3.4.4)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.2.0->spektral) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.2.0->spektral) (3.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow>=2.2.0->spektral) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.2.0->spektral) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.2.0->spektral) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.2.0->spektral) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow>=2.2.0->spektral) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow>=2.2.0->spektral) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.2.0->spektral) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow>=2.2.0->spektral) (3.2.2)\nDownloading spektral-1.3.1-py3-none-any.whl (140 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, spektral\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.24.3\n    Uninstalling numpy-1.24.3:\n      Successfully uninstalled numpy-1.24.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nchex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.22.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ncuml 23.8.0 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\nfeaturetools 1.28.0 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\nfeaturetools 1.28.0 requires scipy>=1.10.0, but you have scipy 1.7.3 which is incompatible.\nfitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 1.3.5 which is incompatible.\ninequality 1.0.1 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\ninequality 1.0.1 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\njax 0.4.21 requires scipy>=1.9, but you have scipy 1.7.3 which is incompatible.\njaxlib 0.4.21+cuda11.cudnn86 requires scipy>=1.9, but you have scipy 1.7.3 which is incompatible.\nkaggle-environments 1.14.3 requires scipy>=1.11.2, but you have scipy 1.7.3 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\nlibpysal 4.9.2 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.4.0 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.22.4 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\nscikit-image 0.21.0 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\nspglm 1.1.0 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\nspglm 1.1.0 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\nspopt 0.6.0 requires pandas!=1.5.0,>=1.4, but you have pandas 1.3.5 which is incompatible.\nspopt 0.6.0 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspreg 1.4.2 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nwoodwork 0.27.0 requires pandas>=1.4.3, but you have pandas 1.3.5 which is incompatible.\nwoodwork 0.27.0 requires scipy>=1.10.0, but you have scipy 1.7.3 which is incompatible.\nxarray 2023.12.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.22.4 spektral-1.3.1\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\ndef scaled_dot_product_attention(q, k, v, mask=None):\n  \"\"\"Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead)\n  but it must be broadcastable for addition.\n\n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable\n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n  Returns:\n    output, attention_weights\n  \"\"\"\n\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)\n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits)#, axis=-1)#tf.nn.linear()#, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output,attention_weights \n\n#multi-head\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self,*, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n\n    assert d_model % self.num_heads == 0\n\n    self.depth = d_model // self.num_heads\n\n    self.wq = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wk = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wv = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n\n    self.dense = tf.keras.layers.Dense(d_model)#,activation='relu',use_bias='true')\n\n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n\n  def call(self, v, k, q, mask=None):\n    batch_size = tf.shape(q)[0]\n\n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n#     q=PReLU()(q)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n#     k=PReLU()(k)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n#     v=PReLU()(v)\n\n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n\n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention,\n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n    return output#, attention_weights\n\n\n\nclass CrossStitch(tf.keras.layers.Layer):\n\n    \"\"\"Cross-Stitch implementation according to arXiv:1604.03539\n    Implementation adapted from https://github.com/helloyide/Cross-stitch-Networks-for-Multi-task-Learning\"\"\"\n\n    def __init__(self, num_tasks, *args, **kwargs):\n        \"\"\"initialize class variables\"\"\"\n        self.num_tasks = num_tasks\n        super(CrossStitch, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        \"\"\"initialize the kernel and set the instance to 'built'\"\"\"\n        self.kernel = self.add_weight(name=\"kernel\",\n                                      shape=(self.num_tasks,\n                                             self.num_tasks),\n                                      initializer='identity',\n                                      trainable=True)\n        super(CrossStitch, self).build(input_shape)\n\n    def call(self, xl):\n        \"\"\"\n        called by TensorFlow when the model gets build. \n        Returns a stacked tensor with num_tasks channels in the 0 dimension, \n        which need to be unstacked.\n        \"\"\"\n        if (len(xl) != self.num_tasks):\n            # should not happen\n            raise ValueError()\n\n        out_values = []\n        for this_task in range(self.num_tasks):\n            this_weight = self.kernel[this_task, this_task]\n            out = tf.math.scalar_mul(this_weight, xl[this_task])\n            for other_task in range(self.num_tasks):\n                if this_task == other_task:\n                    continue  # already weighted!\n                other_weight = self.kernel[this_task, other_task]\n#                 out += tf.math.scalar_mul(other_weight, xl[other_task])\n            out_values.append(out)\n        # HACK!\n        # unless we stack, and then unstack the tensors, TF (2.0.0) can't follow\n        # the graph, so it aborts during model initialization.\n        # return tf.stack(out_values, axis=0)\n        return out_values[0],out_values[1]\n\n    def compute_output_shape(self, input_shape):\n        return [self.num_tasks] + input_shape\n\n    def get_config(self):\n        \"\"\"implemented so keras can save the model to json/yml\"\"\"\n        config = {\n            \"num_tasks\": self.num_tasks\n        }\n        base_config = super(CrossStitch, self).get_config()\n        return dict(list(config.items()) + list(base_config.items()))\n\n    \n \n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:28:18.491122Z","iopub.execute_input":"2024-03-18T13:28:18.491537Z","iopub.status.idle":"2024-03-18T13:28:18.515194Z","shell.execute_reply.started":"2024-03-18T13:28:18.491499Z","shell.execute_reply":"2024-03-18T13:28:18.513944Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\ndef generate_network_att1(drug1,drug2,cell):\n    # fill the architecture params from dict\n\n    \n    cell_layers = [1024,512,128]\n    drop=0.0\n#     snp_layers = [512,128]\n\n#     ddi_layers=[1024,512,256]\n    t_layers=[567,128]\n    g_layers=[254,128]\n#     g_layers=[327,128]\n    dsn1_layers = [394,128]\n    dsn2_layers = [394,128]\n    f_layers=[142,128]\n    \n    c_layers=[1024,512,256]\n    \n    m_layers=[209,128]\n    p_layers=[1024,512,256]\n    \n    l2_reg = 1e-3  # L2 regularization rate\n\n#     p_layers=[drug1.shape[1],512,128]\n\n    dropout=0.0\n\n   \n    # contruct two parallel networks\n   \n    drug1=Input(shape=(drug1.shape[1],))\n    \n    for layer in range(len(p_layers)):\n      if layer == 0:\n\n        train = Dense(int(p_layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(drug1)\n        train = Dropout(float(drop))(train)\n        \n      elif layer == (len(p_layers)-1):\n        drug11 = Dense(int(p_layers[layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(train)\n\n      else:\n        train = Dense(int(p_layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(train)\n        train = Dropout(float(drop))(train)\n        \n        \n    drug2=Input(shape=(drug2.shape[1],))\n    \n    for layer in range(len(p_layers)):\n      if layer == 0:\n\n        train = Dense(int(p_layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(drug2)\n        train = Dropout(float(drop))(train)\n        \n      elif layer == (len(p_layers)-1):\n        drug22 = Dense(int(p_layers[layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(train)\n\n      else:\n        train = Dense(int(p_layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(train)\n        train = Dropout(float(drop))(train)\n        \n        \n    cell=Input(shape=(cell.shape[1],),name='train')\n    \n    for layer in range(len(c_layers)):\n      if layer == 0:\n\n        train = Dense(int(c_layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cell)\n        train = Dropout(float(drop))(train)\n        \n      elif layer == (len(c_layers)-1):\n        cell1 = Dense(int(c_layers[layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(train)\n\n      else:\n        train = Dense(int(c_layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(train)\n        train = Dropout(float(drop))(train)\n \n\n  \n    concatModel=concatenate([drug11,drug22,cell1])\n \n    \n    layer1 =MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task1= layer1(concatModel,concatModel,concatModel)\n    layer2 = MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task2= layer2(concatModel,concatModel,concatModel)\n    task11 = Reshape([a_task1.shape[2]])(a_task1)\n    task22 = Reshape([a_task2.shape[2]])(a_task2)\n    task1=concatenate([task11,concatModel])\n    task2=concatenate([task22,concatModel])\n    \n    r_task1,r_task2 = CrossStitch(2)([task1,task2])\n\n    \n    r_task1=Dense(2048,activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    \n    r_task2=Dense(2048, activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n\n                             \n    r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n\n    \n    r_task1=concatenate([r_task1,task1])\n    r_task2=concatenate([r_task2,task2])\n\n\n    r_task1 = Dense(1024, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(1024, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n    \n    r_task1 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fsynergy1')(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(128, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fclass1')(r_task2)\n\n   \n    r_task1 = Dense(64, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fsynergy2')(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(64, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fclass2')(r_task2)\n\n\n    \n    snp_output1 = Dense(1, activation='linear',name='synergy')(r_task1)\n    snp_output2 = Dense(3, activation='sigmoid',name='class')(r_task2)\n    \n\n    model = Model(inputs=[drug1,drug2,cell],outputs= [snp_output1,snp_output2])\n\n    print(model.summary())\n    return model\n\n\ndef trainer_att1(model, train_d1,train_d2,train_c,train_synergy,train_class, epo, batch_size, earlyStop,test_d1,test_d2,test_c,test_synergy,test_class1):\n\n    optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001)\n\n\n    model.compile(optimizer=optimizer,loss={'synergy':'mse','class':'categorical_crossentropy'})\n\n    model.fit([train_d1,train_d2,train_c],[train_synergy,train_class],shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 )\n              \n\n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:28:18.516950Z","iopub.execute_input":"2024-03-18T13:28:18.517550Z","iopub.status.idle":"2024-03-18T13:28:18.551736Z","shell.execute_reply.started":"2024-03-18T13:28:18.517514Z","shell.execute_reply":"2024-03-18T13:28:18.550785Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# train_d1=[train_f_drug1,train_graph1,train_finger_d1,train_target1]\n# train_d2=[train_f_drug2,train_graph2,train_finger_d2,train_target2]\n# train_c=[train_cell_line,train_copy,train_mutation,train_protematics]\n# test_d1=[test_f_drug1,test_graph1,test_finger_d1,test_target1]\n# test_d2=[test_f_drug2,test_graph2,test_finger_d2,test_target2]\n# test_c=[test_cell_line,test_copy,test_mutation,test_protematics]\n\n\n\ntrain_d1=train_target1\ntrain_d2=train_target2\ntrain_c=train_protematics\ntest_d1=test_target1\ntest_d2=test_target2\ntest_c=test_protematics\n\ntrain_d1=tf.convert_to_tensor(train_d1)\ntest_d1=tf.convert_to_tensor(test_d1)\ntrain_d2=tf.convert_to_tensor(train_d2)\ntest_d2=tf.convert_to_tensor(test_d2)\ntrain_c=tf.convert_to_tensor(train_c)\ntest_c=tf.convert_to_tensor(test_c)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:28:18.554475Z","iopub.execute_input":"2024-03-18T13:28:18.555312Z","iopub.status.idle":"2024-03-18T13:28:28.356021Z","shell.execute_reply.started":"2024-03-18T13:28:18.555275Z","shell.execute_reply":"2024-03-18T13:28:28.354978Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\n   \nmax_epoch =500\nbatch_size = 512 \nearlyStop_patience = 20\n\n\n\n\nmodel_att1= generate_network_att1(train_d1,train_d2,train_c)\n\ntrain_class1 = keras.utils.to_categorical(train_class, num_classes=3)\ntest_class1 = keras.utils.to_categorical(test_class, num_classes=3)\n\nmodel_att1=trainer_att1(model_att1, train_d1,train_d2,train_c,train_synergy,train_class1,max_epoch, batch_size,\n                                earlyStop_patience,test_d1,test_d2,test_c,test_synergy,test_class1)\n\nap111,ap221= model_att1.predict( [test_d1,test_d2,test_c])\n\npositive_negative_indices = np.where(test_class != 2)\nap221=ap221[positive_negative_indices]\ntest_class12=np.array(test_class)\ntest_class12=test_class12[positive_negative_indices]\nap221=ap221[:,1]\n\n\nap11=[]\ntest_synergy1=[]\nl=len(ap111)\nl1=int(l/2)\nfor i in range(l1) :\n    ap=(ap111[i]+ap111[i+l1])/2\n    ap11.append(ap)\n    aap=(test_synergy[i]+test_synergy[i+l1])/2\n    test_synergy1.append(aap)\n    \n    \nap22=[]\ntest_class1=[]\nl=len(ap221)\nl1=int(l/2)\nfor i in range(l1) :\n    ap=(ap221[i]+ap221[i+l1])/2\n    ap22.append(ap)\n    aap=(test_class12[i]+test_class12[i+l1])/2\n    test_class1.append(aap)\n    \n    \n    \nasynergy_error1=mean_squared_error(test_synergy1, ap11)\nasynergy_error11=mean_absolute_error(test_synergy1, ap11)\nasynergy_error21=r2_score(test_synergy1, ap11)\n\nprint(\"msynergy_mean_squared_error\",asynergy_error1)\nprint(\"mclass_mean_squared_error\",asynergy_error11)\nprint(\"msynergy_r2_score\",asynergy_error21)\n\nasynergy_pear1= pearsonr(test_synergy1, ap11)\nasynergy_spear1= spearmanr(test_synergy1, ap11)\nprint(\"msynergy_pear\",asynergy_pear1)\nprint(\"msynergy_spear\",asynergy_spear1)\n\naap2=convert_tobin(ap22)\n\naclass_error1=roc_auc_score(test_class1, ap22)\naclass_error11=accuracy_score(test_class1, aap2)\naclass_error21=cohen_kappa_score(test_class1, aap2)\n\nprint(\"msclass_roc_curve\",aclass_error1)\nprint(\"mclass_accuracy_scorer\",aclass_error11)\nprint(\"mclass_cohen_kappa_score\",aclass_error21)\n\n\naclass_pear1= precision_score(test_class1, aap2)\naclass_spear1= average_precision_score(test_class1, ap22)\nprint(\"mclass_precision_score\",aclass_pear1)\nprint(\"mclass_average_precision_score\",aclass_spear1)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:28:28.357474Z","iopub.execute_input":"2024-03-18T13:28:28.357775Z","iopub.status.idle":"2024-03-18T13:43:09.344960Z","shell.execute_reply.started":"2024-03-18T13:28:28.357751Z","shell.execute_reply":"2024-03-18T13:43:09.343952Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_1 (InputLayer)        [(None, 567)]                0         []                            \n                                                                                                  \n input_2 (InputLayer)        [(None, 567)]                0         []                            \n                                                                                                  \n train (InputLayer)          [(None, 256)]                0         []                            \n                                                                                                  \n dense (Dense)               (None, 1024)                 581632    ['input_1[0][0]']             \n                                                                                                  \n dense_3 (Dense)             (None, 1024)                 581632    ['input_2[0][0]']             \n                                                                                                  \n dense_6 (Dense)             (None, 1024)                 263168    ['train[0][0]']               \n                                                                                                  \n dropout (Dropout)           (None, 1024)                 0         ['dense[0][0]']               \n                                                                                                  \n dropout_2 (Dropout)         (None, 1024)                 0         ['dense_3[0][0]']             \n                                                                                                  \n dropout_4 (Dropout)         (None, 1024)                 0         ['dense_6[0][0]']             \n                                                                                                  \n dense_1 (Dense)             (None, 512)                  524800    ['dropout[0][0]']             \n                                                                                                  \n dense_4 (Dense)             (None, 512)                  524800    ['dropout_2[0][0]']           \n                                                                                                  \n dense_7 (Dense)             (None, 512)                  524800    ['dropout_4[0][0]']           \n                                                                                                  \n dropout_1 (Dropout)         (None, 512)                  0         ['dense_1[0][0]']             \n                                                                                                  \n dropout_3 (Dropout)         (None, 512)                  0         ['dense_4[0][0]']             \n                                                                                                  \n dropout_5 (Dropout)         (None, 512)                  0         ['dense_7[0][0]']             \n                                                                                                  \n dense_2 (Dense)             (None, 256)                  131328    ['dropout_1[0][0]']           \n                                                                                                  \n dense_5 (Dense)             (None, 256)                  131328    ['dropout_3[0][0]']           \n                                                                                                  \n dense_8 (Dense)             (None, 256)                  131328    ['dropout_5[0][0]']           \n                                                                                                  \n concatenate (Concatenate)   (None, 768)                  0         ['dense_2[0][0]',             \n                                                                     'dense_5[0][0]',             \n                                                                     'dense_8[0][0]']             \n                                                                                                  \n multi_head_attention (Mult  (None, None, 768)            2362368   ['concatenate[0][0]',         \n iHeadAttention)                                                     'concatenate[0][0]',         \n                                                                     'concatenate[0][0]']         \n                                                                                                  \n multi_head_attention_1 (Mu  (None, None, 768)            2362368   ['concatenate[0][0]',         \n ltiHeadAttention)                                                   'concatenate[0][0]',         \n                                                                     'concatenate[0][0]']         \n                                                                                                  \n reshape (Reshape)           (None, 768)                  0         ['multi_head_attention[0][0]']\n                                                                                                  \n reshape_1 (Reshape)         (None, 768)                  0         ['multi_head_attention_1[0][0]\n                                                                    ']                            \n                                                                                                  \n concatenate_1 (Concatenate  (None, 1536)                 0         ['reshape[0][0]',             \n )                                                                   'concatenate[0][0]']         \n                                                                                                  \n concatenate_2 (Concatenate  (None, 1536)                 0         ['reshape_1[0][0]',           \n )                                                                   'concatenate[0][0]']         \n                                                                                                  \n cross_stitch (CrossStitch)  ((None, 1536),               4         ['concatenate_1[0][0]',       \n                              (None, 1536))                          'concatenate_2[0][0]']       \n                                                                                                  \n dense_17 (Dense)            (None, 2048)                 3147776   ['cross_stitch[0][0]']        \n                                                                                                  \n dense_18 (Dense)            (None, 2048)                 3147776   ['cross_stitch[0][1]']        \n                                                                                                  \n cross_stitch_1 (CrossStitc  ((None, 2048),               4         ['dense_17[0][0]',            \n h)                           (None, 2048))                          'dense_18[0][0]']            \n                                                                                                  \n concatenate_3 (Concatenate  (None, 3584)                 0         ['cross_stitch_1[0][0]',      \n )                                                                   'concatenate_1[0][0]']       \n                                                                                                  \n dense_19 (Dense)            (None, 1024)                 3671040   ['concatenate_3[0][0]']       \n                                                                                                  \n p_re_lu (PReLU)             (None, 1024)                 1024      ['dense_19[0][0]']            \n                                                                                                  \n fsynergy1 (Dense)           (None, 128)                  131200    ['p_re_lu[0][0]']             \n                                                                                                  \n concatenate_4 (Concatenate  (None, 3584)                 0         ['cross_stitch_1[0][1]',      \n )                                                                   'concatenate_2[0][0]']       \n                                                                                                  \n p_re_lu_1 (PReLU)           (None, 128)                  128       ['fsynergy1[0][0]']           \n                                                                                                  \n dense_20 (Dense)            (None, 1024)                 3671040   ['concatenate_4[0][0]']       \n                                                                                                  \n fsynergy2 (Dense)           (None, 64)                   8256      ['p_re_lu_1[0][0]']           \n                                                                                                  \n fclass1 (Dense)             (None, 128)                  131200    ['dense_20[0][0]']            \n                                                                                                  \n p_re_lu_2 (PReLU)           (None, 64)                   64        ['fsynergy2[0][0]']           \n                                                                                                  \n fclass2 (Dense)             (None, 64)                   8256      ['fclass1[0][0]']             \n                                                                                                  \n synergy (Dense)             (None, 1)                    65        ['p_re_lu_2[0][0]']           \n                                                                                                  \n class (Dense)               (None, 3)                    195       ['fclass2[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 22037580 (84.07 MB)\nTrainable params: 22037580 (84.07 MB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\nNone\nEpoch 1/500\n71/71 [==============================] - 13s 24ms/step - loss: 907.8939 - synergy_loss: 425.5609 - class_loss: 0.8609\nEpoch 2/500\n71/71 [==============================] - 2s 24ms/step - loss: 806.4349 - synergy_loss: 364.0264 - class_loss: 0.7657\nEpoch 3/500\n71/71 [==============================] - 2s 24ms/step - loss: 722.1700 - synergy_loss: 312.3559 - class_loss: 0.7407\nEpoch 4/500\n71/71 [==============================] - 2s 25ms/step - loss: 654.2795 - synergy_loss: 271.1172 - class_loss: 0.7240\nEpoch 5/500\n71/71 [==============================] - 2s 25ms/step - loss: 613.6434 - synergy_loss: 251.6747 - class_loss: 0.7202\nEpoch 6/500\n71/71 [==============================] - 2s 24ms/step - loss: 573.9609 - synergy_loss: 227.7522 - class_loss: 0.7008\nEpoch 7/500\n71/71 [==============================] - 2s 24ms/step - loss: 543.8157 - synergy_loss: 209.5713 - class_loss: 0.6776\nEpoch 8/500\n71/71 [==============================] - 2s 25ms/step - loss: 518.9370 - synergy_loss: 193.1176 - class_loss: 0.6535\nEpoch 9/500\n71/71 [==============================] - 2s 24ms/step - loss: 502.7816 - synergy_loss: 182.8588 - class_loss: 0.6282\nEpoch 10/500\n71/71 [==============================] - 2s 24ms/step - loss: 484.1033 - synergy_loss: 168.5110 - class_loss: 0.6041\nEpoch 11/500\n71/71 [==============================] - 2s 24ms/step - loss: 469.6393 - synergy_loss: 156.8454 - class_loss: 0.5739\nEpoch 12/500\n71/71 [==============================] - 2s 24ms/step - loss: 453.8588 - synergy_loss: 143.0766 - class_loss: 0.5569\nEpoch 13/500\n71/71 [==============================] - 2s 24ms/step - loss: 440.3600 - synergy_loss: 130.8307 - class_loss: 0.5331\nEpoch 14/500\n71/71 [==============================] - 2s 24ms/step - loss: 420.5578 - synergy_loss: 112.1965 - class_loss: 0.5137\nEpoch 15/500\n71/71 [==============================] - 2s 25ms/step - loss: 402.0973 - synergy_loss: 94.8256 - class_loss: 0.4984\nEpoch 16/500\n71/71 [==============================] - 2s 24ms/step - loss: 390.0949 - synergy_loss: 83.9053 - class_loss: 0.4766\nEpoch 17/500\n71/71 [==============================] - 2s 24ms/step - loss: 378.5790 - synergy_loss: 73.5805 - class_loss: 0.4668\nEpoch 18/500\n71/71 [==============================] - 2s 24ms/step - loss: 370.0355 - synergy_loss: 66.1842 - class_loss: 0.4474\nEpoch 19/500\n71/71 [==============================] - 2s 24ms/step - loss: 362.1128 - synergy_loss: 59.3464 - class_loss: 0.4340\nEpoch 20/500\n71/71 [==============================] - 2s 24ms/step - loss: 353.3347 - synergy_loss: 51.6645 - class_loss: 0.4227\nEpoch 21/500\n71/71 [==============================] - 2s 25ms/step - loss: 346.3659 - synergy_loss: 45.8493 - class_loss: 0.4083\nEpoch 22/500\n71/71 [==============================] - 2s 24ms/step - loss: 341.0987 - synergy_loss: 41.7197 - class_loss: 0.3992\nEpoch 23/500\n71/71 [==============================] - 2s 24ms/step - loss: 335.9537 - synergy_loss: 37.7576 - class_loss: 0.3856\nEpoch 24/500\n71/71 [==============================] - 2s 24ms/step - loss: 331.3249 - synergy_loss: 34.2973 - class_loss: 0.3767\nEpoch 25/500\n71/71 [==============================] - 2s 25ms/step - loss: 326.4743 - synergy_loss: 30.6330 - class_loss: 0.3643\nEpoch 26/500\n71/71 [==============================] - 2s 25ms/step - loss: 322.1657 - synergy_loss: 27.5720 - class_loss: 0.3511\nEpoch 27/500\n71/71 [==============================] - 2s 24ms/step - loss: 318.3629 - synergy_loss: 24.9398 - class_loss: 0.3438\nEpoch 28/500\n71/71 [==============================] - 2s 24ms/step - loss: 313.9648 - synergy_loss: 21.7612 - class_loss: 0.3329\nEpoch 29/500\n71/71 [==============================] - 2s 24ms/step - loss: 310.3216 - synergy_loss: 19.3523 - class_loss: 0.3200\nEpoch 30/500\n71/71 [==============================] - 2s 24ms/step - loss: 309.3447 - synergy_loss: 19.6511 - class_loss: 0.3148\nEpoch 31/500\n71/71 [==============================] - 2s 24ms/step - loss: 305.6078 - synergy_loss: 17.1572 - class_loss: 0.3037\nEpoch 32/500\n71/71 [==============================] - 2s 24ms/step - loss: 302.5192 - synergy_loss: 15.4015 - class_loss: 0.2971\nEpoch 33/500\n71/71 [==============================] - 2s 24ms/step - loss: 300.9274 - synergy_loss: 15.1142 - class_loss: 0.2841\nEpoch 34/500\n71/71 [==============================] - 2s 24ms/step - loss: 297.9491 - synergy_loss: 13.4366 - class_loss: 0.2751\nEpoch 35/500\n71/71 [==============================] - 2s 25ms/step - loss: 295.1239 - synergy_loss: 11.9143 - class_loss: 0.2660\nEpoch 36/500\n71/71 [==============================] - 2s 24ms/step - loss: 293.0714 - synergy_loss: 11.2211 - class_loss: 0.2540\nEpoch 37/500\n71/71 [==============================] - 2s 24ms/step - loss: 290.7003 - synergy_loss: 10.2023 - class_loss: 0.2473\nEpoch 38/500\n71/71 [==============================] - 2s 24ms/step - loss: 292.7239 - synergy_loss: 13.5675 - class_loss: 0.2372\nEpoch 39/500\n71/71 [==============================] - 2s 24ms/step - loss: 291.2042 - synergy_loss: 13.3375 - class_loss: 0.2342\nEpoch 40/500\n71/71 [==============================] - 2s 24ms/step - loss: 289.6960 - synergy_loss: 13.1480 - class_loss: 0.2236\nEpoch 41/500\n71/71 [==============================] - 2s 24ms/step - loss: 288.0072 - synergy_loss: 12.8168 - class_loss: 0.2171\nEpoch 42/500\n71/71 [==============================] - 2s 24ms/step - loss: 285.6249 - synergy_loss: 11.7908 - class_loss: 0.2073\nEpoch 43/500\n71/71 [==============================] - 2s 24ms/step - loss: 283.1048 - synergy_loss: 10.6236 - class_loss: 0.1979\nEpoch 44/500\n71/71 [==============================] - 2s 24ms/step - loss: 281.2139 - synergy_loss: 10.0955 - class_loss: 0.1940\nEpoch 45/500\n71/71 [==============================] - 2s 26ms/step - loss: 279.6163 - synergy_loss: 9.8789 - class_loss: 0.1840\nEpoch 46/500\n71/71 [==============================] - 2s 24ms/step - loss: 276.1079 - synergy_loss: 7.7891 - class_loss: 0.1716\nEpoch 47/500\n71/71 [==============================] - 2s 24ms/step - loss: 273.9701 - synergy_loss: 7.0752 - class_loss: 0.1697\nEpoch 48/500\n71/71 [==============================] - 2s 24ms/step - loss: 272.2580 - synergy_loss: 6.8224 - class_loss: 0.1595\nEpoch 49/500\n71/71 [==============================] - 2s 24ms/step - loss: 270.8054 - synergy_loss: 6.8008 - class_loss: 0.1512\nEpoch 50/500\n71/71 [==============================] - 2s 24ms/step - loss: 269.9800 - synergy_loss: 7.4127 - class_loss: 0.1517\nEpoch 51/500\n71/71 [==============================] - 2s 24ms/step - loss: 269.6100 - synergy_loss: 8.4651 - class_loss: 0.1419\nEpoch 52/500\n71/71 [==============================] - 2s 24ms/step - loss: 269.1264 - synergy_loss: 9.3773 - class_loss: 0.1360\nEpoch 53/500\n71/71 [==============================] - 2s 24ms/step - loss: 271.0184 - synergy_loss: 12.5628 - class_loss: 0.1419\nEpoch 54/500\n71/71 [==============================] - 2s 24ms/step - loss: 267.2052 - synergy_loss: 10.1202 - class_loss: 0.1318\nEpoch 55/500\n71/71 [==============================] - 2s 25ms/step - loss: 267.4352 - synergy_loss: 11.7470 - class_loss: 0.1268\nEpoch 56/500\n71/71 [==============================] - 2s 24ms/step - loss: 266.1303 - synergy_loss: 11.7773 - class_loss: 0.1279\nEpoch 57/500\n71/71 [==============================] - 2s 24ms/step - loss: 264.4171 - synergy_loss: 11.4210 - class_loss: 0.1198\nEpoch 58/500\n71/71 [==============================] - 2s 24ms/step - loss: 263.7238 - synergy_loss: 12.0627 - class_loss: 0.1240\nEpoch 59/500\n71/71 [==============================] - 2s 24ms/step - loss: 260.8166 - synergy_loss: 10.4970 - class_loss: 0.1102\nEpoch 60/500\n71/71 [==============================] - 2s 24ms/step - loss: 257.6667 - synergy_loss: 8.7509 - class_loss: 0.1015\nEpoch 61/500\n71/71 [==============================] - 2s 24ms/step - loss: 255.6708 - synergy_loss: 8.1960 - class_loss: 0.0990\nEpoch 62/500\n71/71 [==============================] - 2s 24ms/step - loss: 255.2589 - synergy_loss: 9.1586 - class_loss: 0.0981\nEpoch 63/500\n71/71 [==============================] - 2s 25ms/step - loss: 252.9572 - synergy_loss: 8.2598 - class_loss: 0.0928\nEpoch 64/500\n71/71 [==============================] - 2s 24ms/step - loss: 254.8715 - synergy_loss: 11.5046 - class_loss: 0.0922\nEpoch 65/500\n71/71 [==============================] - 2s 25ms/step - loss: 251.0000 - synergy_loss: 8.9570 - class_loss: 0.0895\nEpoch 66/500\n71/71 [==============================] - 2s 24ms/step - loss: 247.9655 - synergy_loss: 7.3676 - class_loss: 0.0790\nEpoch 67/500\n71/71 [==============================] - 2s 24ms/step - loss: 245.6153 - synergy_loss: 6.4355 - class_loss: 0.0725\nEpoch 68/500\n71/71 [==============================] - 2s 24ms/step - loss: 243.6313 - synergy_loss: 5.9047 - class_loss: 0.0685\nEpoch 69/500\n71/71 [==============================] - 2s 24ms/step - loss: 241.9292 - synergy_loss: 5.6494 - class_loss: 0.0652\nEpoch 70/500\n71/71 [==============================] - 2s 24ms/step - loss: 240.8618 - synergy_loss: 5.9832 - class_loss: 0.0648\nEpoch 71/500\n71/71 [==============================] - 2s 24ms/step - loss: 239.2398 - synergy_loss: 5.8043 - class_loss: 0.0616\nEpoch 72/500\n71/71 [==============================] - 2s 24ms/step - loss: 238.2086 - synergy_loss: 6.2194 - class_loss: 0.0604\nEpoch 73/500\n71/71 [==============================] - 2s 24ms/step - loss: 236.8557 - synergy_loss: 6.2887 - class_loss: 0.0553\nEpoch 74/500\n71/71 [==============================] - 2s 24ms/step - loss: 236.1476 - synergy_loss: 6.9751 - class_loss: 0.0559\nEpoch 75/500\n71/71 [==============================] - 2s 24ms/step - loss: 235.7214 - synergy_loss: 7.8942 - class_loss: 0.0584\nEpoch 76/500\n71/71 [==============================] - 2s 24ms/step - loss: 236.6562 - synergy_loss: 10.1585 - class_loss: 0.0657\nEpoch 77/500\n71/71 [==============================] - 2s 24ms/step - loss: 242.3904 - synergy_loss: 16.8391 - class_loss: 0.0990\nEpoch 78/500\n71/71 [==============================] - 2s 24ms/step - loss: 243.7414 - synergy_loss: 19.0999 - class_loss: 0.1051\nEpoch 79/500\n71/71 [==============================] - 2s 24ms/step - loss: 243.0897 - synergy_loss: 19.5213 - class_loss: 0.1009\nEpoch 80/500\n71/71 [==============================] - 2s 24ms/step - loss: 234.7725 - synergy_loss: 12.4615 - class_loss: 0.0782\nEpoch 81/500\n71/71 [==============================] - 2s 25ms/step - loss: 229.1155 - synergy_loss: 8.2872 - class_loss: 0.0551\nEpoch 82/500\n71/71 [==============================] - 2s 24ms/step - loss: 225.4570 - synergy_loss: 6.1239 - class_loss: 0.0439\nEpoch 83/500\n71/71 [==============================] - 2s 24ms/step - loss: 223.0340 - synergy_loss: 5.1423 - class_loss: 0.0388\nEpoch 84/500\n71/71 [==============================] - 2s 24ms/step - loss: 221.9196 - synergy_loss: 5.4226 - class_loss: 0.0366\nEpoch 85/500\n71/71 [==============================] - 2s 25ms/step - loss: 220.2818 - synergy_loss: 5.1402 - class_loss: 0.0370\nEpoch 86/500\n71/71 [==============================] - 2s 24ms/step - loss: 219.7172 - synergy_loss: 5.8504 - class_loss: 0.0364\nEpoch 87/500\n71/71 [==============================] - 2s 24ms/step - loss: 217.0159 - synergy_loss: 4.5353 - class_loss: 0.0325\nEpoch 88/500\n71/71 [==============================] - 2s 24ms/step - loss: 214.9558 - synergy_loss: 3.9203 - class_loss: 0.0294\nEpoch 89/500\n71/71 [==============================] - 2s 24ms/step - loss: 213.2914 - synergy_loss: 3.6773 - class_loss: 0.0279\nEpoch 90/500\n71/71 [==============================] - 2s 24ms/step - loss: 211.9196 - synergy_loss: 3.7162 - class_loss: 0.0281\nEpoch 91/500\n71/71 [==============================] - 2s 24ms/step - loss: 210.7345 - synergy_loss: 3.9094 - class_loss: 0.0270\nEpoch 92/500\n71/71 [==============================] - 2s 24ms/step - loss: 210.0674 - synergy_loss: 4.6039 - class_loss: 0.0274\nEpoch 93/500\n71/71 [==============================] - 2s 24ms/step - loss: 209.1624 - synergy_loss: 5.0444 - class_loss: 0.0270\nEpoch 94/500\n71/71 [==============================] - 2s 25ms/step - loss: 209.1049 - synergy_loss: 6.2989 - class_loss: 0.0287\nEpoch 95/500\n71/71 [==============================] - 2s 25ms/step - loss: 209.1297 - synergy_loss: 7.5848 - class_loss: 0.0310\nEpoch 96/500\n71/71 [==============================] - 2s 24ms/step - loss: 209.6289 - synergy_loss: 9.2285 - class_loss: 0.0443\nEpoch 97/500\n71/71 [==============================] - 2s 24ms/step - loss: 210.1922 - synergy_loss: 10.9311 - class_loss: 0.0668\nEpoch 98/500\n71/71 [==============================] - 2s 24ms/step - loss: 210.4664 - synergy_loss: 12.3436 - class_loss: 0.0753\nEpoch 99/500\n71/71 [==============================] - 2s 24ms/step - loss: 211.0309 - synergy_loss: 13.9063 - class_loss: 0.0800\nEpoch 100/500\n71/71 [==============================] - 2s 25ms/step - loss: 208.3137 - synergy_loss: 12.3694 - class_loss: 0.0582\nEpoch 101/500\n71/71 [==============================] - 2s 24ms/step - loss: 205.0855 - synergy_loss: 10.4047 - class_loss: 0.0437\nEpoch 102/500\n71/71 [==============================] - 2s 24ms/step - loss: 202.2152 - synergy_loss: 8.8296 - class_loss: 0.0350\nEpoch 103/500\n71/71 [==============================] - 2s 24ms/step - loss: 201.8886 - synergy_loss: 9.6993 - class_loss: 0.0333\nEpoch 104/500\n71/71 [==============================] - 2s 24ms/step - loss: 200.1429 - synergy_loss: 9.1184 - class_loss: 0.0306\nEpoch 105/500\n71/71 [==============================] - 2s 25ms/step - loss: 197.6847 - synergy_loss: 7.8760 - class_loss: 0.0273\nEpoch 106/500\n71/71 [==============================] - 2s 24ms/step - loss: 194.8880 - synergy_loss: 6.3599 - class_loss: 0.0241\nEpoch 107/500\n71/71 [==============================] - 2s 24ms/step - loss: 192.4712 - synergy_loss: 5.2689 - class_loss: 0.0205\nEpoch 108/500\n71/71 [==============================] - 2s 24ms/step - loss: 191.5789 - synergy_loss: 5.6341 - class_loss: 0.0192\nEpoch 109/500\n71/71 [==============================] - 2s 24ms/step - loss: 190.0178 - synergy_loss: 5.3097 - class_loss: 0.0182\nEpoch 110/500\n71/71 [==============================] - 2s 24ms/step - loss: 188.5703 - synergy_loss: 5.0946 - class_loss: 0.0179\nEpoch 111/500\n71/71 [==============================] - 2s 24ms/step - loss: 186.5733 - synergy_loss: 4.3753 - class_loss: 0.0168\nEpoch 112/500\n71/71 [==============================] - 2s 24ms/step - loss: 185.1148 - synergy_loss: 4.2072 - class_loss: 0.0166\nEpoch 113/500\n71/71 [==============================] - 2s 24ms/step - loss: 184.0063 - synergy_loss: 4.3312 - class_loss: 0.0166\nEpoch 114/500\n71/71 [==============================] - 2s 24ms/step - loss: 183.0400 - synergy_loss: 4.5859 - class_loss: 0.0166\nEpoch 115/500\n71/71 [==============================] - 2s 24ms/step - loss: 182.6317 - synergy_loss: 5.3764 - class_loss: 0.0168\nEpoch 116/500\n71/71 [==============================] - 2s 24ms/step - loss: 182.5958 - synergy_loss: 6.4623 - class_loss: 0.0178\nEpoch 117/500\n71/71 [==============================] - 2s 24ms/step - loss: 183.3251 - synergy_loss: 8.2371 - class_loss: 0.0228\nEpoch 118/500\n71/71 [==============================] - 2s 25ms/step - loss: 182.8247 - synergy_loss: 8.7733 - class_loss: 0.0326\nEpoch 119/500\n71/71 [==============================] - 2s 24ms/step - loss: 181.5954 - synergy_loss: 8.6021 - class_loss: 0.0585\nEpoch 120/500\n71/71 [==============================] - 2s 24ms/step - loss: 181.7693 - synergy_loss: 9.8349 - class_loss: 0.0619\nEpoch 121/500\n71/71 [==============================] - 2s 24ms/step - loss: 182.8964 - synergy_loss: 11.8741 - class_loss: 0.0600\nEpoch 122/500\n71/71 [==============================] - 2s 24ms/step - loss: 182.1989 - synergy_loss: 12.0727 - class_loss: 0.0536\nEpoch 123/500\n71/71 [==============================] - 2s 24ms/step - loss: 180.3178 - synergy_loss: 11.1945 - class_loss: 0.0461\nEpoch 124/500\n71/71 [==============================] - 2s 24ms/step - loss: 178.0351 - synergy_loss: 10.0088 - class_loss: 0.0369\nEpoch 125/500\n71/71 [==============================] - 2s 25ms/step - loss: 175.9148 - synergy_loss: 9.0146 - class_loss: 0.0264\nEpoch 126/500\n71/71 [==============================] - 2s 24ms/step - loss: 175.2827 - synergy_loss: 9.4331 - class_loss: 0.0244\nEpoch 127/500\n71/71 [==============================] - 2s 24ms/step - loss: 171.8562 - synergy_loss: 7.1834 - class_loss: 0.0180\nEpoch 128/500\n71/71 [==============================] - 2s 24ms/step - loss: 169.2714 - synergy_loss: 5.7708 - class_loss: 0.0144\nEpoch 129/500\n71/71 [==============================] - 2s 24ms/step - loss: 167.0002 - synergy_loss: 4.7224 - class_loss: 0.0125\nEpoch 130/500\n71/71 [==============================] - 2s 24ms/step - loss: 164.5949 - synergy_loss: 3.5509 - class_loss: 0.0115\nEpoch 131/500\n71/71 [==============================] - 2s 24ms/step - loss: 162.8537 - synergy_loss: 3.0049 - class_loss: 0.0112\nEpoch 132/500\n71/71 [==============================] - 2s 24ms/step - loss: 162.3572 - synergy_loss: 3.6093 - class_loss: 0.0116\nEpoch 133/500\n71/71 [==============================] - 2s 24ms/step - loss: 161.5187 - synergy_loss: 3.8649 - class_loss: 0.0117\nEpoch 134/500\n71/71 [==============================] - 2s 25ms/step - loss: 160.6850 - synergy_loss: 4.1020 - class_loss: 0.0121\nEpoch 135/500\n71/71 [==============================] - 2s 25ms/step - loss: 160.6002 - synergy_loss: 5.0486 - class_loss: 0.0126\nEpoch 136/500\n71/71 [==============================] - 2s 25ms/step - loss: 159.5439 - synergy_loss: 5.0337 - class_loss: 0.0125\nEpoch 137/500\n71/71 [==============================] - 2s 25ms/step - loss: 158.0766 - synergy_loss: 4.6785 - class_loss: 0.0118\nEpoch 138/500\n71/71 [==============================] - 2s 24ms/step - loss: 156.5226 - synergy_loss: 4.2551 - class_loss: 0.0114\nEpoch 139/500\n71/71 [==============================] - 2s 24ms/step - loss: 156.2430 - synergy_loss: 5.0490 - class_loss: 0.0113\nEpoch 140/500\n71/71 [==============================] - 2s 24ms/step - loss: 155.2053 - synergy_loss: 5.0621 - class_loss: 0.0121\nEpoch 141/500\n71/71 [==============================] - 2s 24ms/step - loss: 154.7050 - synergy_loss: 5.5890 - class_loss: 0.0122\nEpoch 142/500\n71/71 [==============================] - 2s 24ms/step - loss: 155.1233 - synergy_loss: 6.9487 - class_loss: 0.0144\nEpoch 143/500\n71/71 [==============================] - 2s 24ms/step - loss: 156.1235 - synergy_loss: 8.7327 - class_loss: 0.0210\nEpoch 144/500\n71/71 [==============================] - 2s 24ms/step - loss: 156.7841 - synergy_loss: 10.1279 - class_loss: 0.0803\nEpoch 145/500\n71/71 [==============================] - 2s 25ms/step - loss: 156.9158 - synergy_loss: 11.0790 - class_loss: 0.0898\nEpoch 146/500\n71/71 [==============================] - 2s 24ms/step - loss: 156.1068 - synergy_loss: 11.0876 - class_loss: 0.0512\nEpoch 147/500\n71/71 [==============================] - 2s 24ms/step - loss: 156.6990 - synergy_loss: 12.4605 - class_loss: 0.0515\nEpoch 148/500\n71/71 [==============================] - 2s 24ms/step - loss: 151.9435 - synergy_loss: 8.7220 - class_loss: 0.0289\nEpoch 149/500\n71/71 [==============================] - 2s 24ms/step - loss: 150.9776 - synergy_loss: 8.7835 - class_loss: 0.0190\nEpoch 150/500\n71/71 [==============================] - 2s 24ms/step - loss: 149.9854 - synergy_loss: 8.7168 - class_loss: 0.0171\nEpoch 151/500\n71/71 [==============================] - 2s 24ms/step - loss: 146.6877 - synergy_loss: 6.4391 - class_loss: 0.0123\nEpoch 152/500\n71/71 [==============================] - 2s 24ms/step - loss: 144.1126 - synergy_loss: 4.9615 - class_loss: 0.0103\nEpoch 153/500\n71/71 [==============================] - 2s 24ms/step - loss: 141.6876 - synergy_loss: 3.6692 - class_loss: 0.0088\nEpoch 154/500\n71/71 [==============================] - 2s 25ms/step - loss: 140.9981 - synergy_loss: 3.9836 - class_loss: 0.0089\nEpoch 155/500\n71/71 [==============================] - 2s 25ms/step - loss: 140.2064 - synergy_loss: 4.1245 - class_loss: 0.0090\nEpoch 156/500\n71/71 [==============================] - 2s 24ms/step - loss: 138.9865 - synergy_loss: 3.8606 - class_loss: 0.0088\nEpoch 157/500\n71/71 [==============================] - 2s 24ms/step - loss: 137.8896 - synergy_loss: 3.7368 - class_loss: 0.0088\nEpoch 158/500\n71/71 [==============================] - 2s 24ms/step - loss: 136.5182 - synergy_loss: 3.3626 - class_loss: 0.0088\nEpoch 159/500\n71/71 [==============================] - 2s 24ms/step - loss: 135.0240 - synergy_loss: 2.8872 - class_loss: 0.0084\nEpoch 160/500\n71/71 [==============================] - 2s 24ms/step - loss: 134.7904 - synergy_loss: 3.5739 - class_loss: 0.0088\nEpoch 161/500\n71/71 [==============================] - 2s 24ms/step - loss: 134.7248 - synergy_loss: 4.3690 - class_loss: 0.0092\nEpoch 162/500\n71/71 [==============================] - 2s 24ms/step - loss: 135.4330 - synergy_loss: 5.8733 - class_loss: 0.0098\nEpoch 163/500\n71/71 [==============================] - 2s 24ms/step - loss: 137.1673 - synergy_loss: 8.2040 - class_loss: 0.0137\nEpoch 164/500\n71/71 [==============================] - 2s 24ms/step - loss: 138.8640 - synergy_loss: 10.4468 - class_loss: 0.0390\nEpoch 165/500\n71/71 [==============================] - 2s 24ms/step - loss: 137.6441 - synergy_loss: 9.8726 - class_loss: 0.0573\nEpoch 166/500\n71/71 [==============================] - 2s 25ms/step - loss: 136.1802 - synergy_loss: 9.2896 - class_loss: 0.0462\nEpoch 167/500\n71/71 [==============================] - 2s 24ms/step - loss: 135.1941 - synergy_loss: 9.0927 - class_loss: 0.0336\nEpoch 168/500\n71/71 [==============================] - 2s 23ms/step - loss: 131.0072 - synergy_loss: 5.9373 - class_loss: 0.0207\nEpoch 169/500\n71/71 [==============================] - 2s 24ms/step - loss: 128.5505 - synergy_loss: 4.5713 - class_loss: 0.0116\nEpoch 170/500\n71/71 [==============================] - 2s 24ms/step - loss: 126.9632 - synergy_loss: 3.9821 - class_loss: 0.0082\nEpoch 171/500\n71/71 [==============================] - 2s 24ms/step - loss: 127.0907 - synergy_loss: 4.9834 - class_loss: 0.0076\nEpoch 172/500\n71/71 [==============================] - 2s 25ms/step - loss: 127.3718 - synergy_loss: 5.9716 - class_loss: 0.0087\nEpoch 173/500\n71/71 [==============================] - 2s 25ms/step - loss: 125.9599 - synergy_loss: 5.3166 - class_loss: 0.0086\nEpoch 174/500\n71/71 [==============================] - 2s 24ms/step - loss: 124.7384 - synergy_loss: 4.9609 - class_loss: 0.0079\nEpoch 175/500\n71/71 [==============================] - 2s 24ms/step - loss: 123.4157 - synergy_loss: 4.5269 - class_loss: 0.0074\nEpoch 176/500\n71/71 [==============================] - 2s 24ms/step - loss: 122.4301 - synergy_loss: 4.4048 - class_loss: 0.0075\nEpoch 177/500\n71/71 [==============================] - 2s 24ms/step - loss: 122.2274 - synergy_loss: 4.9865 - class_loss: 0.0079\nEpoch 178/500\n71/71 [==============================] - 2s 24ms/step - loss: 122.4821 - synergy_loss: 5.9353 - class_loss: 0.0079\nEpoch 179/500\n71/71 [==============================] - 2s 24ms/step - loss: 122.5807 - synergy_loss: 6.6899 - class_loss: 0.0091\nEpoch 180/500\n71/71 [==============================] - 2s 24ms/step - loss: 122.3711 - synergy_loss: 7.1642 - class_loss: 0.0101\nEpoch 181/500\n71/71 [==============================] - 2s 24ms/step - loss: 121.8697 - synergy_loss: 7.3544 - class_loss: 0.0095\nEpoch 182/500\n71/71 [==============================] - 2s 24ms/step - loss: 121.2352 - synergy_loss: 7.4175 - class_loss: 0.0115\nEpoch 183/500\n71/71 [==============================] - 2s 24ms/step - loss: 119.8414 - synergy_loss: 6.7961 - class_loss: 0.0175\nEpoch 184/500\n71/71 [==============================] - 2s 25ms/step - loss: 117.8330 - synergy_loss: 5.6475 - class_loss: 0.0220\nEpoch 185/500\n71/71 [==============================] - 2s 24ms/step - loss: 116.3821 - synergy_loss: 5.0246 - class_loss: 0.0296\nEpoch 186/500\n71/71 [==============================] - 2s 24ms/step - loss: 115.0501 - synergy_loss: 4.4961 - class_loss: 0.0357\nEpoch 187/500\n71/71 [==============================] - 2s 24ms/step - loss: 114.4673 - synergy_loss: 4.7067 - class_loss: 0.0220\nEpoch 188/500\n71/71 [==============================] - 2s 24ms/step - loss: 114.4000 - synergy_loss: 5.3470 - class_loss: 0.0123\nEpoch 189/500\n71/71 [==============================] - 2s 24ms/step - loss: 113.4609 - synergy_loss: 5.1341 - class_loss: 0.0087\nEpoch 190/500\n71/71 [==============================] - 2s 24ms/step - loss: 112.8161 - synergy_loss: 5.1839 - class_loss: 0.0067\nEpoch 191/500\n71/71 [==============================] - 2s 25ms/step - loss: 112.3548 - synergy_loss: 5.4444 - class_loss: 0.0069\nEpoch 192/500\n71/71 [==============================] - 2s 24ms/step - loss: 111.9378 - synergy_loss: 5.7150 - class_loss: 0.0070\nEpoch 193/500\n71/71 [==============================] - 2s 24ms/step - loss: 112.0762 - synergy_loss: 6.4445 - class_loss: 0.0084\nEpoch 194/500\n71/71 [==============================] - 2s 24ms/step - loss: 111.1269 - synergy_loss: 6.1404 - class_loss: 0.0079\nEpoch 195/500\n71/71 [==============================] - 2s 25ms/step - loss: 109.3355 - synergy_loss: 5.1632 - class_loss: 0.0063\nEpoch 196/500\n71/71 [==============================] - 2s 24ms/step - loss: 108.1054 - synergy_loss: 4.6976 - class_loss: 0.0060\nEpoch 197/500\n71/71 [==============================] - 2s 25ms/step - loss: 107.4334 - synergy_loss: 4.7599 - class_loss: 0.0059\nEpoch 198/500\n71/71 [==============================] - 2s 24ms/step - loss: 106.1267 - synergy_loss: 4.1864 - class_loss: 0.0057\nEpoch 199/500\n71/71 [==============================] - 2s 24ms/step - loss: 105.7529 - synergy_loss: 4.5254 - class_loss: 0.0060\nEpoch 200/500\n71/71 [==============================] - 2s 24ms/step - loss: 104.3950 - synergy_loss: 3.8914 - class_loss: 0.0056\nEpoch 201/500\n71/71 [==============================] - 2s 25ms/step - loss: 104.1550 - synergy_loss: 4.3470 - class_loss: 0.0060\nEpoch 202/500\n71/71 [==============================] - 2s 24ms/step - loss: 104.1488 - synergy_loss: 4.9310 - class_loss: 0.0064\nEpoch 203/500\n71/71 [==============================] - 2s 24ms/step - loss: 104.0943 - synergy_loss: 5.4550 - class_loss: 0.0066\nEpoch 204/500\n71/71 [==============================] - 2s 24ms/step - loss: 104.2635 - synergy_loss: 6.1762 - class_loss: 0.0072\nEpoch 205/500\n71/71 [==============================] - 2s 24ms/step - loss: 103.8283 - synergy_loss: 6.3750 - class_loss: 0.0066\nEpoch 206/500\n71/71 [==============================] - 2s 24ms/step - loss: 104.1961 - synergy_loss: 7.2130 - class_loss: 0.0145\nEpoch 207/500\n71/71 [==============================] - 2s 24ms/step - loss: 102.7835 - synergy_loss: 6.3265 - class_loss: 0.0985\nEpoch 208/500\n71/71 [==============================] - 2s 24ms/step - loss: 101.6786 - synergy_loss: 5.9662 - class_loss: 0.0456\nEpoch 209/500\n71/71 [==============================] - 2s 24ms/step - loss: 100.9924 - synergy_loss: 5.9670 - class_loss: 0.0185\nEpoch 210/500\n71/71 [==============================] - 2s 24ms/step - loss: 100.8287 - synergy_loss: 6.3462 - class_loss: 0.0133\nEpoch 211/500\n71/71 [==============================] - 2s 24ms/step - loss: 100.4346 - synergy_loss: 6.5295 - class_loss: 0.0105\nEpoch 212/500\n71/71 [==============================] - 2s 24ms/step - loss: 100.7799 - synergy_loss: 7.3731 - class_loss: 0.0097\nEpoch 213/500\n71/71 [==============================] - 2s 24ms/step - loss: 100.7677 - synergy_loss: 7.8380 - class_loss: 0.0092\nEpoch 214/500\n71/71 [==============================] - 2s 24ms/step - loss: 99.3890 - synergy_loss: 7.0477 - class_loss: 0.0074\nEpoch 215/500\n71/71 [==============================] - 2s 24ms/step - loss: 98.8185 - synergy_loss: 7.0025 - class_loss: 0.0070\nEpoch 216/500\n71/71 [==============================] - 2s 24ms/step - loss: 96.9144 - synergy_loss: 5.7629 - class_loss: 0.0055\nEpoch 217/500\n71/71 [==============================] - 2s 24ms/step - loss: 95.4135 - synergy_loss: 4.9481 - class_loss: 0.0054\nEpoch 218/500\n71/71 [==============================] - 2s 24ms/step - loss: 94.4206 - synergy_loss: 4.5959 - class_loss: 0.0051\nEpoch 219/500\n71/71 [==============================] - 2s 24ms/step - loss: 93.1712 - synergy_loss: 4.0225 - class_loss: 0.0047\nEpoch 220/500\n71/71 [==============================] - 2s 24ms/step - loss: 91.9262 - synergy_loss: 3.4228 - class_loss: 0.0047\nEpoch 221/500\n71/71 [==============================] - 2s 24ms/step - loss: 91.7808 - synergy_loss: 3.8341 - class_loss: 0.0051\nEpoch 222/500\n71/71 [==============================] - 2s 24ms/step - loss: 91.2605 - synergy_loss: 3.8639 - class_loss: 0.0051\nEpoch 223/500\n71/71 [==============================] - 2s 24ms/step - loss: 92.1856 - synergy_loss: 5.2895 - class_loss: 0.0057\nEpoch 224/500\n71/71 [==============================] - 2s 25ms/step - loss: 92.5892 - synergy_loss: 6.0192 - class_loss: 0.0065\nEpoch 225/500\n71/71 [==============================] - 2s 24ms/step - loss: 91.3087 - synergy_loss: 5.2773 - class_loss: 0.0058\nEpoch 226/500\n71/71 [==============================] - 2s 24ms/step - loss: 90.4731 - synergy_loss: 5.0282 - class_loss: 0.0051\nEpoch 227/500\n71/71 [==============================] - 2s 24ms/step - loss: 89.4796 - synergy_loss: 4.6144 - class_loss: 0.0050\nEpoch 228/500\n71/71 [==============================] - 2s 24ms/step - loss: 88.2216 - synergy_loss: 3.9863 - class_loss: 0.0048\nEpoch 229/500\n71/71 [==============================] - 2s 24ms/step - loss: 87.0876 - synergy_loss: 3.4730 - class_loss: 0.0046\nEpoch 230/500\n71/71 [==============================] - 2s 24ms/step - loss: 86.1580 - synergy_loss: 3.1545 - class_loss: 0.0044\nEpoch 231/500\n71/71 [==============================] - 2s 24ms/step - loss: 85.1538 - synergy_loss: 2.7647 - class_loss: 0.0043\nEpoch 232/500\n71/71 [==============================] - 2s 24ms/step - loss: 84.4013 - synergy_loss: 2.6128 - class_loss: 0.0044\nEpoch 233/500\n71/71 [==============================] - 2s 24ms/step - loss: 84.0995 - synergy_loss: 2.8696 - class_loss: 0.0046\nEpoch 234/500\n71/71 [==============================] - 2s 24ms/step - loss: 84.1560 - synergy_loss: 3.3996 - class_loss: 0.0047\nEpoch 235/500\n71/71 [==============================] - 2s 24ms/step - loss: 83.9630 - synergy_loss: 3.6941 - class_loss: 0.0049\nEpoch 236/500\n71/71 [==============================] - 2s 24ms/step - loss: 84.7735 - synergy_loss: 4.8852 - class_loss: 0.0056\nEpoch 237/500\n71/71 [==============================] - 2s 25ms/step - loss: 85.8292 - synergy_loss: 6.2821 - class_loss: 0.0067\nEpoch 238/500\n71/71 [==============================] - 2s 24ms/step - loss: 86.1145 - synergy_loss: 6.8516 - class_loss: 0.0728\nEpoch 239/500\n71/71 [==============================] - 2s 24ms/step - loss: 87.1896 - synergy_loss: 8.2289 - class_loss: 0.0924\nEpoch 240/500\n71/71 [==============================] - 2s 24ms/step - loss: 85.8952 - synergy_loss: 7.3994 - class_loss: 0.0352\nEpoch 241/500\n71/71 [==============================] - 2s 24ms/step - loss: 84.5008 - synergy_loss: 6.5706 - class_loss: 0.0197\nEpoch 242/500\n71/71 [==============================] - 2s 24ms/step - loss: 83.4409 - synergy_loss: 6.0375 - class_loss: 0.0143\nEpoch 243/500\n71/71 [==============================] - 2s 24ms/step - loss: 81.9487 - synergy_loss: 5.1051 - class_loss: 0.0068\nEpoch 244/500\n71/71 [==============================] - 2s 25ms/step - loss: 80.9951 - synergy_loss: 4.7154 - class_loss: 0.0055\nEpoch 245/500\n71/71 [==============================] - 2s 24ms/step - loss: 79.5177 - synergy_loss: 3.8083 - class_loss: 0.0047\nEpoch 246/500\n71/71 [==============================] - 2s 25ms/step - loss: 79.2062 - synergy_loss: 4.0153 - class_loss: 0.0045\nEpoch 247/500\n71/71 [==============================] - 2s 24ms/step - loss: 78.9880 - synergy_loss: 4.2497 - class_loss: 0.0045\nEpoch 248/500\n71/71 [==============================] - 2s 24ms/step - loss: 78.2725 - synergy_loss: 3.9985 - class_loss: 0.0046\nEpoch 249/500\n71/71 [==============================] - 2s 24ms/step - loss: 77.4219 - synergy_loss: 3.6506 - class_loss: 0.0044\nEpoch 250/500\n71/71 [==============================] - 2s 24ms/step - loss: 77.6837 - synergy_loss: 4.3249 - class_loss: 0.0047\nEpoch 251/500\n71/71 [==============================] - 2s 24ms/step - loss: 78.5398 - synergy_loss: 5.4840 - class_loss: 0.0051\nEpoch 252/500\n71/71 [==============================] - 2s 24ms/step - loss: 79.0209 - synergy_loss: 6.2586 - class_loss: 0.0056\nEpoch 253/500\n71/71 [==============================] - 2s 24ms/step - loss: 77.5329 - synergy_loss: 5.2387 - class_loss: 0.0050\nEpoch 254/500\n71/71 [==============================] - 2s 25ms/step - loss: 76.5275 - synergy_loss: 4.7624 - class_loss: 0.0044\nEpoch 255/500\n71/71 [==============================] - 2s 25ms/step - loss: 75.6837 - synergy_loss: 4.3737 - class_loss: 0.0045\nEpoch 256/500\n71/71 [==============================] - 2s 24ms/step - loss: 74.7743 - synergy_loss: 3.9448 - class_loss: 0.0044\nEpoch 257/500\n71/71 [==============================] - 2s 24ms/step - loss: 73.7792 - synergy_loss: 3.4457 - class_loss: 0.0040\nEpoch 258/500\n71/71 [==============================] - 2s 24ms/step - loss: 73.9419 - synergy_loss: 4.0366 - class_loss: 0.0042\nEpoch 259/500\n71/71 [==============================] - 2s 24ms/step - loss: 74.4566 - synergy_loss: 4.8874 - class_loss: 0.0046\nEpoch 260/500\n71/71 [==============================] - 2s 24ms/step - loss: 74.2257 - synergy_loss: 4.9806 - class_loss: 0.0047\nEpoch 261/500\n71/71 [==============================] - 2s 24ms/step - loss: 74.8156 - synergy_loss: 5.9030 - class_loss: 0.0052\nEpoch 262/500\n71/71 [==============================] - 2s 24ms/step - loss: 74.2557 - synergy_loss: 5.6968 - class_loss: 0.0053\nEpoch 263/500\n71/71 [==============================] - 2s 24ms/step - loss: 73.5376 - synergy_loss: 5.3735 - class_loss: 0.0060\nEpoch 264/500\n71/71 [==============================] - 2s 25ms/step - loss: 72.3013 - synergy_loss: 4.6183 - class_loss: 0.0044\nEpoch 265/500\n71/71 [==============================] - 2s 25ms/step - loss: 72.0969 - synergy_loss: 4.8394 - class_loss: 0.0042\nEpoch 266/500\n71/71 [==============================] - 2s 24ms/step - loss: 70.8819 - synergy_loss: 4.0495 - class_loss: 0.0041\nEpoch 267/500\n71/71 [==============================] - 2s 24ms/step - loss: 69.3822 - synergy_loss: 3.0839 - class_loss: 0.0035\nEpoch 268/500\n71/71 [==============================] - 2s 24ms/step - loss: 68.8374 - synergy_loss: 2.9912 - class_loss: 0.0036\nEpoch 269/500\n71/71 [==============================] - 2s 24ms/step - loss: 68.1613 - synergy_loss: 2.7749 - class_loss: 0.0036\nEpoch 270/500\n71/71 [==============================] - 2s 24ms/step - loss: 68.0214 - synergy_loss: 3.0352 - class_loss: 0.0037\nEpoch 271/500\n71/71 [==============================] - 2s 24ms/step - loss: 68.0017 - synergy_loss: 3.3673 - class_loss: 0.0039\nEpoch 272/500\n71/71 [==============================] - 2s 24ms/step - loss: 70.1906 - synergy_loss: 5.7854 - class_loss: 0.0049\nEpoch 273/500\n71/71 [==============================] - 2s 25ms/step - loss: 78.1280 - synergy_loss: 13.1051 - class_loss: 0.1171\nEpoch 274/500\n71/71 [==============================] - 2s 25ms/step - loss: 76.3644 - synergy_loss: 11.4365 - class_loss: 0.0847\nEpoch 275/500\n71/71 [==============================] - 2s 24ms/step - loss: 72.2480 - synergy_loss: 7.9094 - class_loss: 0.0339\nEpoch 276/500\n71/71 [==============================] - 2s 24ms/step - loss: 69.3856 - synergy_loss: 5.6121 - class_loss: 0.0162\nEpoch 277/500\n71/71 [==============================] - 2s 24ms/step - loss: 67.0523 - synergy_loss: 3.9008 - class_loss: 0.0072\nEpoch 278/500\n71/71 [==============================] - 2s 24ms/step - loss: 65.4044 - synergy_loss: 2.8290 - class_loss: 0.0044\nEpoch 279/500\n71/71 [==============================] - 2s 24ms/step - loss: 64.0279 - synergy_loss: 1.9940 - class_loss: 0.0038\nEpoch 280/500\n71/71 [==============================] - 2s 24ms/step - loss: 63.0722 - synergy_loss: 1.5375 - class_loss: 0.0036\nEpoch 281/500\n71/71 [==============================] - 2s 24ms/step - loss: 62.3689 - synergy_loss: 1.3013 - class_loss: 0.0036\nEpoch 282/500\n71/71 [==============================] - 2s 24ms/step - loss: 61.9117 - synergy_loss: 1.2659 - class_loss: 0.0036\nEpoch 283/500\n71/71 [==============================] - 2s 25ms/step - loss: 61.5501 - synergy_loss: 1.3074 - class_loss: 0.0036\nEpoch 284/500\n71/71 [==============================] - 2s 25ms/step - loss: 61.2148 - synergy_loss: 1.3537 - class_loss: 0.0037\nEpoch 285/500\n71/71 [==============================] - 2s 24ms/step - loss: 60.8990 - synergy_loss: 1.4231 - class_loss: 0.0037\nEpoch 286/500\n71/71 [==============================] - 2s 24ms/step - loss: 61.0820 - synergy_loss: 1.9277 - class_loss: 0.0039\nEpoch 287/500\n71/71 [==============================] - 2s 24ms/step - loss: 61.3599 - synergy_loss: 2.5012 - class_loss: 0.0040\nEpoch 288/500\n71/71 [==============================] - 2s 24ms/step - loss: 63.6472 - synergy_loss: 4.8603 - class_loss: 0.0050\nEpoch 289/500\n71/71 [==============================] - 2s 24ms/step - loss: 65.3219 - synergy_loss: 6.5891 - class_loss: 0.0068\nEpoch 290/500\n71/71 [==============================] - 2s 24ms/step - loss: 67.1052 - synergy_loss: 8.4747 - class_loss: 0.0078\nEpoch 291/500\n71/71 [==============================] - 2s 24ms/step - loss: 69.5646 - synergy_loss: 10.9783 - class_loss: 0.0098\nEpoch 292/500\n71/71 [==============================] - 2s 24ms/step - loss: 67.2454 - synergy_loss: 8.8159 - class_loss: 0.0319\nEpoch 293/500\n71/71 [==============================] - 2s 24ms/step - loss: 63.7748 - synergy_loss: 5.8450 - class_loss: 0.0474\nEpoch 294/500\n71/71 [==============================] - 2s 25ms/step - loss: 61.6276 - synergy_loss: 4.2343 - class_loss: 0.0193\nEpoch 295/500\n71/71 [==============================] - 2s 24ms/step - loss: 60.7998 - synergy_loss: 3.8837 - class_loss: 0.0058\nEpoch 296/500\n71/71 [==============================] - 2s 24ms/step - loss: 59.8375 - synergy_loss: 3.3465 - class_loss: 0.0039\nEpoch 297/500\n71/71 [==============================] - 2s 24ms/step - loss: 59.2427 - synergy_loss: 3.1280 - class_loss: 0.0035\nEpoch 298/500\n71/71 [==============================] - 2s 24ms/step - loss: 58.0236 - synergy_loss: 2.3179 - class_loss: 0.0032\nEpoch 299/500\n71/71 [==============================] - 2s 24ms/step - loss: 57.1094 - synergy_loss: 1.8420 - class_loss: 0.0031\nEpoch 300/500\n71/71 [==============================] - 2s 24ms/step - loss: 56.5064 - synergy_loss: 1.6352 - class_loss: 0.0031\nEpoch 301/500\n71/71 [==============================] - 2s 25ms/step - loss: 56.1723 - synergy_loss: 1.6579 - class_loss: 0.0032\nEpoch 302/500\n71/71 [==============================] - 2s 24ms/step - loss: 55.8239 - synergy_loss: 1.6593 - class_loss: 0.0032\nEpoch 303/500\n71/71 [==============================] - 2s 24ms/step - loss: 55.6036 - synergy_loss: 1.7696 - class_loss: 0.0033\nEpoch 304/500\n71/71 [==============================] - 2s 25ms/step - loss: 55.6903 - synergy_loss: 2.1472 - class_loss: 0.0034\nEpoch 305/500\n71/71 [==============================] - 2s 24ms/step - loss: 56.3157 - synergy_loss: 2.9723 - class_loss: 0.0038\nEpoch 306/500\n71/71 [==============================] - 2s 24ms/step - loss: 57.5343 - synergy_loss: 4.3362 - class_loss: 0.0043\nEpoch 307/500\n71/71 [==============================] - 2s 24ms/step - loss: 59.9527 - synergy_loss: 6.7579 - class_loss: 0.0058\nEpoch 308/500\n71/71 [==============================] - 2s 24ms/step - loss: 62.4167 - synergy_loss: 9.1796 - class_loss: 0.0099\nEpoch 309/500\n71/71 [==============================] - 2s 24ms/step - loss: 61.1474 - synergy_loss: 8.0635 - class_loss: 0.0180\nEpoch 310/500\n71/71 [==============================] - 2s 24ms/step - loss: 59.8161 - synergy_loss: 7.0492 - class_loss: 0.0368\nEpoch 311/500\n71/71 [==============================] - 2s 24ms/step - loss: 57.9869 - synergy_loss: 5.6190 - class_loss: 0.0318\nEpoch 312/500\n71/71 [==============================] - 2s 24ms/step - loss: 56.2506 - synergy_loss: 4.2952 - class_loss: 0.0126\nEpoch 313/500\n71/71 [==============================] - 2s 24ms/step - loss: 54.6796 - synergy_loss: 3.1830 - class_loss: 0.0049\nEpoch 314/500\n71/71 [==============================] - 2s 25ms/step - loss: 53.9837 - synergy_loss: 2.8644 - class_loss: 0.0033\nEpoch 315/500\n71/71 [==============================] - 2s 24ms/step - loss: 53.7033 - synergy_loss: 2.9331 - class_loss: 0.0030\nEpoch 316/500\n71/71 [==============================] - 2s 24ms/step - loss: 53.4166 - synergy_loss: 2.9261 - class_loss: 0.0031\nEpoch 317/500\n71/71 [==============================] - 2s 24ms/step - loss: 53.1399 - synergy_loss: 2.9323 - class_loss: 0.0032\nEpoch 318/500\n71/71 [==============================] - 2s 24ms/step - loss: 52.8185 - synergy_loss: 2.9057 - class_loss: 0.0031\nEpoch 319/500\n71/71 [==============================] - 2s 25ms/step - loss: 52.4246 - synergy_loss: 2.8038 - class_loss: 0.0031\nEpoch 320/500\n71/71 [==============================] - 2s 24ms/step - loss: 52.0146 - synergy_loss: 2.7092 - class_loss: 0.0031\nEpoch 321/500\n71/71 [==============================] - 2s 24ms/step - loss: 52.9021 - synergy_loss: 3.7793 - class_loss: 0.0035\nEpoch 322/500\n71/71 [==============================] - 2s 24ms/step - loss: 53.2225 - synergy_loss: 4.2614 - class_loss: 0.0037\nEpoch 323/500\n71/71 [==============================] - 2s 25ms/step - loss: 53.5720 - synergy_loss: 4.7899 - class_loss: 0.0039\nEpoch 324/500\n71/71 [==============================] - 2s 25ms/step - loss: 52.9287 - synergy_loss: 4.4261 - class_loss: 0.0037\nEpoch 325/500\n71/71 [==============================] - 2s 24ms/step - loss: 54.5757 - synergy_loss: 6.1814 - class_loss: 0.0048\nEpoch 326/500\n71/71 [==============================] - 2s 24ms/step - loss: 54.3795 - synergy_loss: 6.1049 - class_loss: 0.0060\nEpoch 327/500\n71/71 [==============================] - 2s 24ms/step - loss: 53.0974 - synergy_loss: 5.1302 - class_loss: 0.0040\nEpoch 328/500\n71/71 [==============================] - 2s 24ms/step - loss: 52.2348 - synergy_loss: 4.5530 - class_loss: 0.0036\nEpoch 329/500\n71/71 [==============================] - 2s 24ms/step - loss: 51.4896 - synergy_loss: 4.1321 - class_loss: 0.0032\nEpoch 330/500\n71/71 [==============================] - 2s 24ms/step - loss: 50.2756 - synergy_loss: 3.2703 - class_loss: 0.0029\nEpoch 331/500\n71/71 [==============================] - 2s 24ms/step - loss: 49.3666 - synergy_loss: 2.6942 - class_loss: 0.0029\nEpoch 332/500\n71/71 [==============================] - 2s 24ms/step - loss: 48.8274 - synergy_loss: 2.4873 - class_loss: 0.0028\nEpoch 333/500\n71/71 [==============================] - 2s 24ms/step - loss: 48.7948 - synergy_loss: 2.7122 - class_loss: 0.0031\nEpoch 334/500\n71/71 [==============================] - 2s 25ms/step - loss: 49.2491 - synergy_loss: 3.3775 - class_loss: 0.0033\nEpoch 335/500\n71/71 [==============================] - 2s 24ms/step - loss: 58.3251 - synergy_loss: 11.9161 - class_loss: 0.0551\nEpoch 336/500\n71/71 [==============================] - 2s 24ms/step - loss: 67.8143 - synergy_loss: 20.5566 - class_loss: 0.1312\nEpoch 337/500\n71/71 [==============================] - 2s 24ms/step - loss: 59.2922 - synergy_loss: 12.0624 - class_loss: 0.0536\nEpoch 338/500\n71/71 [==============================] - 2s 24ms/step - loss: 52.8713 - synergy_loss: 6.3105 - class_loss: 0.0121\nEpoch 339/500\n71/71 [==============================] - 2s 24ms/step - loss: 49.7994 - synergy_loss: 3.8331 - class_loss: 0.0052\nEpoch 340/500\n71/71 [==============================] - 2s 24ms/step - loss: 47.8500 - synergy_loss: 2.3996 - class_loss: 0.0039\nEpoch 341/500\n71/71 [==============================] - 2s 24ms/step - loss: 46.5183 - synergy_loss: 1.5274 - class_loss: 0.0034\nEpoch 342/500\n71/71 [==============================] - 2s 24ms/step - loss: 45.6751 - synergy_loss: 1.0958 - class_loss: 0.0032\nEpoch 343/500\n71/71 [==============================] - 2s 24ms/step - loss: 45.0874 - synergy_loss: 0.8706 - class_loss: 0.0032\nEpoch 344/500\n71/71 [==============================] - 2s 25ms/step - loss: 44.4884 - synergy_loss: 0.6235 - class_loss: 0.0031\nEpoch 345/500\n71/71 [==============================] - 2s 24ms/step - loss: 43.9854 - synergy_loss: 0.4556 - class_loss: 0.0031\nEpoch 346/500\n71/71 [==============================] - 2s 24ms/step - loss: 43.6284 - synergy_loss: 0.4104 - class_loss: 0.0031\nEpoch 347/500\n71/71 [==============================] - 2s 24ms/step - loss: 43.3231 - synergy_loss: 0.3981 - class_loss: 0.0031\nEpoch 348/500\n71/71 [==============================] - 2s 24ms/step - loss: 43.0096 - synergy_loss: 0.3708 - class_loss: 0.0031\nEpoch 349/500\n71/71 [==============================] - 2s 24ms/step - loss: 42.7895 - synergy_loss: 0.4213 - class_loss: 0.0032\nEpoch 350/500\n71/71 [==============================] - 2s 24ms/step - loss: 42.7034 - synergy_loss: 0.5784 - class_loss: 0.0032\nEpoch 351/500\n71/71 [==============================] - 2s 24ms/step - loss: 42.6131 - synergy_loss: 0.7247 - class_loss: 0.0032\nEpoch 352/500\n71/71 [==============================] - 2s 24ms/step - loss: 42.7917 - synergy_loss: 1.1007 - class_loss: 0.0034\nEpoch 353/500\n71/71 [==============================] - 2s 25ms/step - loss: 43.6069 - synergy_loss: 2.0301 - class_loss: 0.0036\nEpoch 354/500\n71/71 [==============================] - 2s 24ms/step - loss: 44.5924 - synergy_loss: 3.0775 - class_loss: 0.0039\nEpoch 355/500\n71/71 [==============================] - 2s 24ms/step - loss: 46.3898 - synergy_loss: 4.8900 - class_loss: 0.0045\nEpoch 356/500\n71/71 [==============================] - 2s 25ms/step - loss: 48.4594 - synergy_loss: 6.9106 - class_loss: 0.0057\nEpoch 357/500\n71/71 [==============================] - 2s 24ms/step - loss: 49.8341 - synergy_loss: 8.2891 - class_loss: 0.0067\nEpoch 358/500\n71/71 [==============================] - 2s 24ms/step - loss: 50.8404 - synergy_loss: 9.3245 - class_loss: 0.0084\nEpoch 359/500\n71/71 [==============================] - 2s 24ms/step - loss: 49.9546 - synergy_loss: 8.5487 - class_loss: 0.0133\nEpoch 360/500\n71/71 [==============================] - 2s 24ms/step - loss: 47.2008 - synergy_loss: 6.0913 - class_loss: 0.0216\nEpoch 361/500\n71/71 [==============================] - 2s 24ms/step - loss: 44.9966 - synergy_loss: 4.2583 - class_loss: 0.0233\nEpoch 362/500\n71/71 [==============================] - 2s 24ms/step - loss: 43.9034 - synergy_loss: 3.5228 - class_loss: 0.0134\nEpoch 363/500\n71/71 [==============================] - 2s 24ms/step - loss: 42.6772 - synergy_loss: 2.6586 - class_loss: 0.0044\nEpoch 364/500\n71/71 [==============================] - 2s 25ms/step - loss: 42.3852 - synergy_loss: 2.6448 - class_loss: 0.0029\nEpoch 365/500\n71/71 [==============================] - 2s 24ms/step - loss: 44.1438 - synergy_loss: 4.4961 - class_loss: 0.0033\nEpoch 366/500\n71/71 [==============================] - 2s 24ms/step - loss: 44.0995 - synergy_loss: 4.5379 - class_loss: 0.0033\nEpoch 367/500\n71/71 [==============================] - 2s 24ms/step - loss: 43.0338 - synergy_loss: 3.6955 - class_loss: 0.0031\nEpoch 368/500\n71/71 [==============================] - 2s 24ms/step - loss: 42.5363 - synergy_loss: 3.4573 - class_loss: 0.0029\nEpoch 369/500\n71/71 [==============================] - 2s 24ms/step - loss: 41.4062 - synergy_loss: 2.6118 - class_loss: 0.0027\nEpoch 370/500\n71/71 [==============================] - 2s 24ms/step - loss: 40.7284 - synergy_loss: 2.2317 - class_loss: 0.0026\nEpoch 371/500\n71/71 [==============================] - 2s 24ms/step - loss: 40.2777 - synergy_loss: 2.0309 - class_loss: 0.0027\nEpoch 372/500\n71/71 [==============================] - 2s 24ms/step - loss: 39.6949 - synergy_loss: 1.7131 - class_loss: 0.0027\nEpoch 373/500\n71/71 [==============================] - 2s 24ms/step - loss: 39.4761 - synergy_loss: 1.7348 - class_loss: 0.0028\nEpoch 374/500\n71/71 [==============================] - 2s 25ms/step - loss: 39.2889 - synergy_loss: 1.7570 - class_loss: 0.0028\nEpoch 375/500\n71/71 [==============================] - 2s 24ms/step - loss: 39.0728 - synergy_loss: 1.7409 - class_loss: 0.0029\nEpoch 376/500\n71/71 [==============================] - 2s 24ms/step - loss: 39.1214 - synergy_loss: 1.9945 - class_loss: 0.0029\nEpoch 377/500\n71/71 [==============================] - 2s 24ms/step - loss: 39.3825 - synergy_loss: 2.4131 - class_loss: 0.0031\nEpoch 378/500\n71/71 [==============================] - 2s 24ms/step - loss: 40.8282 - synergy_loss: 3.8691 - class_loss: 0.0037\nEpoch 379/500\n71/71 [==============================] - 2s 24ms/step - loss: 41.4702 - synergy_loss: 4.5566 - class_loss: 0.0037\nEpoch 380/500\n71/71 [==============================] - 2s 24ms/step - loss: 44.3282 - synergy_loss: 7.4609 - class_loss: 0.0050\nEpoch 381/500\n71/71 [==============================] - 2s 24ms/step - loss: 51.3334 - synergy_loss: 13.7454 - class_loss: 0.0984\nEpoch 382/500\n71/71 [==============================] - 2s 24ms/step - loss: 48.1980 - synergy_loss: 10.6737 - class_loss: 0.0493\nEpoch 383/500\n71/71 [==============================] - 2s 25ms/step - loss: 44.9205 - synergy_loss: 7.7504 - class_loss: 0.0146\nEpoch 384/500\n71/71 [==============================] - 2s 25ms/step - loss: 41.8101 - synergy_loss: 5.0112 - class_loss: 0.0070\nEpoch 385/500\n71/71 [==============================] - 2s 24ms/step - loss: 39.4860 - synergy_loss: 3.1228 - class_loss: 0.0037\nEpoch 386/500\n71/71 [==============================] - 2s 24ms/step - loss: 38.4285 - synergy_loss: 2.4264 - class_loss: 0.0030\nEpoch 387/500\n71/71 [==============================] - 2s 24ms/step - loss: 37.2631 - synergy_loss: 1.6046 - class_loss: 0.0028\nEpoch 388/500\n71/71 [==============================] - 2s 24ms/step - loss: 36.5537 - synergy_loss: 1.1919 - class_loss: 0.0027\nEpoch 389/500\n71/71 [==============================] - 2s 24ms/step - loss: 35.9791 - synergy_loss: 0.9106 - class_loss: 0.0027\nEpoch 390/500\n71/71 [==============================] - 2s 24ms/step - loss: 35.5896 - synergy_loss: 0.7844 - class_loss: 0.0027\nEpoch 391/500\n71/71 [==============================] - 2s 24ms/step - loss: 35.2168 - synergy_loss: 0.6594 - class_loss: 0.0027\nEpoch 392/500\n71/71 [==============================] - 2s 25ms/step - loss: 34.9999 - synergy_loss: 0.6649 - class_loss: 0.0028\nEpoch 393/500\n71/71 [==============================] - 2s 25ms/step - loss: 34.8240 - synergy_loss: 0.6996 - class_loss: 0.0028\nEpoch 394/500\n71/71 [==============================] - 2s 25ms/step - loss: 34.8058 - synergy_loss: 0.8624 - class_loss: 0.0029\nEpoch 395/500\n71/71 [==============================] - 2s 24ms/step - loss: 34.8544 - synergy_loss: 1.0813 - class_loss: 0.0029\nEpoch 396/500\n71/71 [==============================] - 2s 24ms/step - loss: 34.9717 - synergy_loss: 1.3464 - class_loss: 0.0030\nEpoch 397/500\n71/71 [==============================] - 2s 24ms/step - loss: 35.2467 - synergy_loss: 1.7487 - class_loss: 0.0032\nEpoch 398/500\n71/71 [==============================] - 2s 24ms/step - loss: 36.3739 - synergy_loss: 2.9377 - class_loss: 0.0034\nEpoch 399/500\n71/71 [==============================] - 2s 24ms/step - loss: 37.9625 - synergy_loss: 4.4949 - class_loss: 0.0039\nEpoch 400/500\n71/71 [==============================] - 2s 24ms/step - loss: 39.3775 - synergy_loss: 5.8717 - class_loss: 0.0044\nEpoch 401/500\n71/71 [==============================] - 2s 24ms/step - loss: 39.7997 - synergy_loss: 6.3206 - class_loss: 0.0045\nEpoch 402/500\n71/71 [==============================] - 2s 24ms/step - loss: 39.7838 - synergy_loss: 6.3954 - class_loss: 0.0043\nEpoch 403/500\n71/71 [==============================] - 2s 24ms/step - loss: 39.7794 - synergy_loss: 6.5006 - class_loss: 0.0043\nEpoch 404/500\n71/71 [==============================] - 2s 25ms/step - loss: 39.1059 - synergy_loss: 5.9739 - class_loss: 0.0036\nEpoch 405/500\n71/71 [==============================] - 2s 24ms/step - loss: 38.2945 - synergy_loss: 5.3599 - class_loss: 0.0032\nEpoch 406/500\n71/71 [==============================] - 2s 24ms/step - loss: 37.4996 - synergy_loss: 4.7559 - class_loss: 0.0028\nEpoch 407/500\n71/71 [==============================] - 2s 24ms/step - loss: 35.8167 - synergy_loss: 3.3499 - class_loss: 0.0025\nEpoch 408/500\n71/71 [==============================] - 2s 24ms/step - loss: 34.5175 - synergy_loss: 2.3429 - class_loss: 0.0024\nEpoch 409/500\n71/71 [==============================] - 2s 24ms/step - loss: 33.7599 - synergy_loss: 1.8472 - class_loss: 0.0024\nEpoch 410/500\n71/71 [==============================] - 2s 24ms/step - loss: 33.2683 - synergy_loss: 1.5960 - class_loss: 0.0024\nEpoch 411/500\n71/71 [==============================] - 2s 25ms/step - loss: 32.8289 - synergy_loss: 1.3750 - class_loss: 0.0024\nEpoch 412/500\n71/71 [==============================] - 2s 25ms/step - loss: 32.6251 - synergy_loss: 1.3711 - class_loss: 0.0025\nEpoch 413/500\n71/71 [==============================] - 2s 25ms/step - loss: 32.5635 - synergy_loss: 1.4722 - class_loss: 0.0026\nEpoch 414/500\n71/71 [==============================] - 2s 24ms/step - loss: 32.3650 - synergy_loss: 1.4457 - class_loss: 0.0027\nEpoch 415/500\n71/71 [==============================] - 2s 24ms/step - loss: 32.6201 - synergy_loss: 1.8363 - class_loss: 0.0028\nEpoch 416/500\n71/71 [==============================] - 2s 25ms/step - loss: 32.9153 - synergy_loss: 2.2178 - class_loss: 0.0029\nEpoch 417/500\n71/71 [==============================] - 2s 24ms/step - loss: 33.4459 - synergy_loss: 2.8321 - class_loss: 0.0030\nEpoch 418/500\n71/71 [==============================] - 2s 24ms/step - loss: 35.2859 - synergy_loss: 4.6396 - class_loss: 0.0035\nEpoch 419/500\n71/71 [==============================] - 2s 24ms/step - loss: 37.0174 - synergy_loss: 6.3058 - class_loss: 0.0054\nEpoch 420/500\n71/71 [==============================] - 2s 24ms/step - loss: 37.8123 - synergy_loss: 6.9360 - class_loss: 0.0858\nEpoch 421/500\n71/71 [==============================] - 2s 24ms/step - loss: 36.5519 - synergy_loss: 5.8773 - class_loss: 0.0472\nEpoch 422/500\n71/71 [==============================] - 2s 24ms/step - loss: 35.1630 - synergy_loss: 4.7588 - class_loss: 0.0127\nEpoch 423/500\n71/71 [==============================] - 2s 24ms/step - loss: 34.0132 - synergy_loss: 3.8418 - class_loss: 0.0053\nEpoch 424/500\n71/71 [==============================] - 2s 25ms/step - loss: 33.2513 - synergy_loss: 3.3027 - class_loss: 0.0034\nEpoch 425/500\n71/71 [==============================] - 2s 24ms/step - loss: 32.3541 - synergy_loss: 2.6212 - class_loss: 0.0030\nEpoch 426/500\n71/71 [==============================] - 2s 25ms/step - loss: 32.3683 - synergy_loss: 2.7794 - class_loss: 0.0030\nEpoch 427/500\n71/71 [==============================] - 2s 25ms/step - loss: 31.6753 - synergy_loss: 2.2916 - class_loss: 0.0028\nEpoch 428/500\n71/71 [==============================] - 2s 24ms/step - loss: 31.3156 - synergy_loss: 2.1161 - class_loss: 0.0027\nEpoch 429/500\n71/71 [==============================] - 2s 26ms/step - loss: 30.9081 - synergy_loss: 1.8925 - class_loss: 0.0026\nEpoch 430/500\n71/71 [==============================] - 2s 25ms/step - loss: 30.7902 - synergy_loss: 1.9234 - class_loss: 0.0027\nEpoch 431/500\n71/71 [==============================] - 2s 25ms/step - loss: 30.6010 - synergy_loss: 1.9015 - class_loss: 0.0026\nEpoch 432/500\n71/71 [==============================] - 2s 25ms/step - loss: 30.3823 - synergy_loss: 1.8385 - class_loss: 0.0027\nEpoch 433/500\n71/71 [==============================] - 2s 25ms/step - loss: 30.3327 - synergy_loss: 1.9217 - class_loss: 0.0027\nEpoch 434/500\n71/71 [==============================] - 2s 25ms/step - loss: 30.1325 - synergy_loss: 1.8863 - class_loss: 0.0027\nEpoch 435/500\n71/71 [==============================] - 2s 24ms/step - loss: 30.3200 - synergy_loss: 2.1837 - class_loss: 0.0028\nEpoch 436/500\n71/71 [==============================] - 2s 24ms/step - loss: 30.7683 - synergy_loss: 2.7122 - class_loss: 0.0029\nEpoch 437/500\n71/71 [==============================] - 2s 24ms/step - loss: 31.0146 - synergy_loss: 3.0279 - class_loss: 0.0029\nEpoch 438/500\n71/71 [==============================] - 2s 24ms/step - loss: 31.3232 - synergy_loss: 3.4277 - class_loss: 0.0030\nEpoch 439/500\n71/71 [==============================] - 2s 24ms/step - loss: 32.8552 - synergy_loss: 4.9257 - class_loss: 0.0036\nEpoch 440/500\n71/71 [==============================] - 2s 24ms/step - loss: 33.4616 - synergy_loss: 5.5275 - class_loss: 0.0037\nEpoch 441/500\n71/71 [==============================] - 2s 24ms/step - loss: 33.8257 - synergy_loss: 5.9405 - class_loss: 0.0037\nEpoch 442/500\n71/71 [==============================] - 2s 24ms/step - loss: 33.9918 - synergy_loss: 6.1157 - class_loss: 0.0083\nEpoch 443/500\n71/71 [==============================] - 2s 24ms/step - loss: 32.7142 - synergy_loss: 4.9966 - class_loss: 0.0131\nEpoch 444/500\n71/71 [==============================] - 2s 25ms/step - loss: 31.4557 - synergy_loss: 3.9497 - class_loss: 0.0259\nEpoch 445/500\n71/71 [==============================] - 2s 24ms/step - loss: 30.9950 - synergy_loss: 3.6493 - class_loss: 0.0305\nEpoch 446/500\n71/71 [==============================] - 2s 24ms/step - loss: 30.2029 - synergy_loss: 3.0533 - class_loss: 0.0159\nEpoch 447/500\n71/71 [==============================] - 2s 24ms/step - loss: 29.2535 - synergy_loss: 2.3396 - class_loss: 0.0041\nEpoch 448/500\n71/71 [==============================] - 2s 24ms/step - loss: 28.7122 - synergy_loss: 1.9898 - class_loss: 0.0024\nEpoch 449/500\n71/71 [==============================] - 2s 24ms/step - loss: 28.5457 - synergy_loss: 1.9776 - class_loss: 0.0022\nEpoch 450/500\n71/71 [==============================] - 2s 24ms/step - loss: 28.0695 - synergy_loss: 1.6734 - class_loss: 0.0022\nEpoch 451/500\n71/71 [==============================] - 2s 24ms/step - loss: 28.1659 - synergy_loss: 1.9010 - class_loss: 0.0023\nEpoch 452/500\n71/71 [==============================] - 2s 24ms/step - loss: 28.2394 - synergy_loss: 2.0708 - class_loss: 0.0023\nEpoch 453/500\n71/71 [==============================] - 2s 24ms/step - loss: 28.6442 - synergy_loss: 2.5680 - class_loss: 0.0024\nEpoch 454/500\n71/71 [==============================] - 2s 24ms/step - loss: 29.7378 - synergy_loss: 3.6475 - class_loss: 0.0027\nEpoch 455/500\n71/71 [==============================] - 2s 24ms/step - loss: 30.1686 - synergy_loss: 4.0688 - class_loss: 0.0034\nEpoch 456/500\n71/71 [==============================] - 2s 25ms/step - loss: 29.6093 - synergy_loss: 3.6469 - class_loss: 0.0026\nEpoch 457/500\n71/71 [==============================] - 2s 24ms/step - loss: 29.7460 - synergy_loss: 3.8603 - class_loss: 0.0028\nEpoch 458/500\n71/71 [==============================] - 2s 24ms/step - loss: 29.0465 - synergy_loss: 3.3195 - class_loss: 0.0025\nEpoch 459/500\n71/71 [==============================] - 2s 24ms/step - loss: 28.0215 - synergy_loss: 2.4916 - class_loss: 0.0023\nEpoch 460/500\n71/71 [==============================] - 2s 24ms/step - loss: 27.3794 - synergy_loss: 2.0281 - class_loss: 0.0022\nEpoch 461/500\n71/71 [==============================] - 2s 24ms/step - loss: 27.0732 - synergy_loss: 1.8893 - class_loss: 0.0022\nEpoch 462/500\n71/71 [==============================] - 2s 24ms/step - loss: 26.7505 - synergy_loss: 1.7190 - class_loss: 0.0022\nEpoch 463/500\n71/71 [==============================] - 2s 25ms/step - loss: 26.6646 - synergy_loss: 1.7627 - class_loss: 0.0023\nEpoch 464/500\n71/71 [==============================] - 2s 24ms/step - loss: 26.4193 - synergy_loss: 1.6540 - class_loss: 0.0023\nEpoch 465/500\n71/71 [==============================] - 2s 24ms/step - loss: 26.5600 - synergy_loss: 1.8878 - class_loss: 0.0024\nEpoch 466/500\n71/71 [==============================] - 2s 25ms/step - loss: 26.8790 - synergy_loss: 2.2861 - class_loss: 0.0025\nEpoch 467/500\n71/71 [==============================] - 2s 24ms/step - loss: 27.7228 - synergy_loss: 3.1404 - class_loss: 0.0027\nEpoch 468/500\n71/71 [==============================] - 2s 24ms/step - loss: 28.3924 - synergy_loss: 3.8103 - class_loss: 0.0029\nEpoch 469/500\n71/71 [==============================] - 2s 24ms/step - loss: 28.5411 - synergy_loss: 4.0269 - class_loss: 0.0028\nEpoch 470/500\n71/71 [==============================] - 2s 24ms/step - loss: 28.6441 - synergy_loss: 4.1905 - class_loss: 0.0028\nEpoch 471/500\n71/71 [==============================] - 2s 24ms/step - loss: 28.0007 - synergy_loss: 3.6426 - class_loss: 0.0025\nEpoch 472/500\n71/71 [==============================] - 2s 24ms/step - loss: 28.4379 - synergy_loss: 4.1673 - class_loss: 0.0028\nEpoch 473/500\n71/71 [==============================] - 2s 25ms/step - loss: 28.1172 - synergy_loss: 3.9408 - class_loss: 0.0026\nEpoch 474/500\n71/71 [==============================] - 2s 24ms/step - loss: 28.1228 - synergy_loss: 4.0419 - class_loss: 0.0025\nEpoch 475/500\n71/71 [==============================] - 2s 24ms/step - loss: 28.2664 - synergy_loss: 4.2721 - class_loss: 0.0025\nEpoch 476/500\n71/71 [==============================] - 2s 24ms/step - loss: 27.4846 - synergy_loss: 3.5800 - class_loss: 0.0023\nEpoch 477/500\n71/71 [==============================] - 2s 24ms/step - loss: 26.4726 - synergy_loss: 2.7532 - class_loss: 0.0022\nEpoch 478/500\n71/71 [==============================] - 2s 24ms/step - loss: 26.1190 - synergy_loss: 2.5497 - class_loss: 0.0022\nEpoch 479/500\n71/71 [==============================] - 2s 24ms/step - loss: 25.6080 - synergy_loss: 2.1858 - class_loss: 0.0021\nEpoch 480/500\n71/71 [==============================] - 2s 24ms/step - loss: 25.0916 - synergy_loss: 1.8257 - class_loss: 0.0021\nEpoch 481/500\n71/71 [==============================] - 2s 24ms/step - loss: 24.8268 - synergy_loss: 1.6915 - class_loss: 0.0021\nEpoch 482/500\n71/71 [==============================] - 2s 24ms/step - loss: 24.8043 - synergy_loss: 1.7917 - class_loss: 0.0022\nEpoch 483/500\n71/71 [==============================] - 2s 24ms/step - loss: 24.7575 - synergy_loss: 1.8439 - class_loss: 0.0022\nEpoch 484/500\n71/71 [==============================] - 2s 25ms/step - loss: 24.6856 - synergy_loss: 1.8684 - class_loss: 0.0023\nEpoch 485/500\n71/71 [==============================] - 2s 24ms/step - loss: 24.4169 - synergy_loss: 1.7267 - class_loss: 0.0022\nEpoch 486/500\n71/71 [==============================] - 2s 24ms/step - loss: 24.4095 - synergy_loss: 1.8206 - class_loss: 0.0022\nEpoch 487/500\n71/71 [==============================] - 2s 24ms/step - loss: 24.6804 - synergy_loss: 2.1531 - class_loss: 0.0023\nEpoch 488/500\n71/71 [==============================] - 2s 24ms/step - loss: 25.0284 - synergy_loss: 2.5581 - class_loss: 0.0025\nEpoch 489/500\n71/71 [==============================] - 2s 24ms/step - loss: 25.8940 - synergy_loss: 3.4212 - class_loss: 0.0027\nEpoch 490/500\n71/71 [==============================] - 2s 24ms/step - loss: 26.8198 - synergy_loss: 4.3175 - class_loss: 0.0029\nEpoch 491/500\n71/71 [==============================] - 2s 24ms/step - loss: 27.0028 - synergy_loss: 4.4912 - class_loss: 0.0222\nEpoch 492/500\n71/71 [==============================] - 2s 24ms/step - loss: 27.4236 - synergy_loss: 4.8928 - class_loss: 0.0956\nEpoch 493/500\n71/71 [==============================] - 2s 24ms/step - loss: 27.2828 - synergy_loss: 4.8625 - class_loss: 0.0206\nEpoch 494/500\n71/71 [==============================] - 2s 24ms/step - loss: 26.1670 - synergy_loss: 3.8769 - class_loss: 0.0073\nEpoch 495/500\n71/71 [==============================] - 2s 24ms/step - loss: 25.5895 - synergy_loss: 3.4567 - class_loss: 0.0039\nEpoch 496/500\n71/71 [==============================] - 2s 24ms/step - loss: 25.2606 - synergy_loss: 3.2489 - class_loss: 0.0027\nEpoch 497/500\n71/71 [==============================] - 2s 24ms/step - loss: 24.1088 - synergy_loss: 2.2796 - class_loss: 0.0024\nEpoch 498/500\n71/71 [==============================] - 2s 24ms/step - loss: 23.6463 - synergy_loss: 1.9703 - class_loss: 0.0023\nEpoch 499/500\n71/71 [==============================] - 2s 24ms/step - loss: 23.4611 - synergy_loss: 1.9102 - class_loss: 0.0023\nEpoch 500/500\n71/71 [==============================] - 2s 24ms/step - loss: 23.5719 - synergy_loss: 2.0987 - class_loss: 0.0023\n291/291 [==============================] - 2s 4ms/step\nmsynergy_mean_squared_error 188.5579\nmclass_mean_squared_error 9.016518\nmsynergy_r2_score 0.644284801170267\nmsynergy_pear (array([0.8036239913854323], dtype=object), 0.0)\nmsynergy_spear SpearmanrResult(correlation=0.7679752169136729, pvalue=0.0)\nmsclass_roc_curve 0.9560919423684718\nmclass_accuracy_scorer 0.9011494252873563\nmclass_cohen_kappa_score 0.6662150770498017\nmclass_precision_score 0.9364548494983278\nmclass_average_precision_score 0.8947857096696427\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink \n\nnp.savetxt('tarpro_cls4.csv', ap22 ,delimiter=',')\nFileLink(r'tarpro_cls4.csv')    ","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:43:09.346320Z","iopub.execute_input":"2024-03-18T13:43:09.346629Z","iopub.status.idle":"2024-03-18T13:43:09.361736Z","shell.execute_reply.started":"2024-03-18T13:43:09.346604Z","shell.execute_reply":"2024-03-18T13:43:09.360899Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/tarpro_cls4.csv","text/html":"<a href='tarpro_cls4.csv' target='_blank'>tarpro_cls4.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"np.savetxt('tarpro_syn4.csv', ap11 ,delimiter=',')\nFileLink(r'tarpro_syn4.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:43:09.362806Z","iopub.execute_input":"2024-03-18T13:43:09.363100Z","iopub.status.idle":"2024-03-18T13:43:09.384534Z","shell.execute_reply.started":"2024-03-18T13:43:09.363075Z","shell.execute_reply":"2024-03-18T13:43:09.383700Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/tarpro_syn4.csv","text/html":"<a href='tarpro_syn4.csv' target='_blank'>tarpro_syn4.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nprint(tf.__version__)\nimport keras as kr\nprint(kr.__version__)\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\nimport platform\nprint(platform.python_version())\n\n\nprint(np.__version__)\nprint(pd.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:43:09.385547Z","iopub.execute_input":"2024-03-18T13:43:09.385854Z","iopub.status.idle":"2024-03-18T13:43:09.395512Z","shell.execute_reply.started":"2024-03-18T13:43:09.385829Z","shell.execute_reply":"2024-03-18T13:43:09.394576Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"2.13.0\n2.13.1\n[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 13521567510330604109\nxla_global_id: -1\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nmemory_limit: 16274030592\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 16858313825578029128\nphysical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\nxla_global_id: 416903419\n]\n3.10.12\n1.24.3\n1.3.5\n","output_type":"stream"}]}]}